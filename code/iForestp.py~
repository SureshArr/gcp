import numpy as np
import sklearn as sk
import matplotlib.pyplot as plt
import pandas as pd
from numpy.random import normal
import graphlab as gl


def _h(i):
    return np.log(i) + 0.5772156649 


def _c(n):
    if n > 2:
        h = _h(n-1)
        return 2*h - 2*(n - 1)/n
    if n == 2:
        return 1
    else:
        return 0


def _anomaly_score(score, n_samples):

    score = -score/_c(n_samples)

    return 2**score


@profile
def _split_data(X):
    ''' split the data in the left and right nodes ''' 
    n_samples, n_columns = X.shape
    n_features = n_columns - 1
    m = M = 0
    while m == M:
        feature_id = np.random.randint(low=0, high=n_features)
        feature = X[:, feature_id]
        m = feature.min()
        M = feature.max()
        #print(m, M, feature_id, X.shape)

    split_value = np.random.uniform(m, M, 1)
    lefties = feature <= split_value
    left_X = X[lefties]
    right_X = X[~lefties]
    return left_X, right_X, feature_id, split_value


def iTree(X, add_index=False, max_depth = np.inf):            
    ''' construct an isolation tree and returns the number of steps required
    to isolate an element. A column of index is added to the input matrix X if  
    add_index=True. This column is required in the algorithm. ''' 

    n_split = {} 
    def iterate(X, count = 0):

        n_samples, n_columns = X.shape
        n_features = n_columns - 1

        if count > max_depth:
            for index in X[:,-1]:
                n_split[index] = count
            return

        if n_samples == 1:
            index = X[0, n_columns-1]
            n_split[index] = count
            return 
        else:
            lX, rX, feature_id, split_value = _split_data(X)
            # Uncomment the print to visualize a draft of 
            # the construction of the tree
            print(lX[:,-1], rX[:,-1], feature_id, split_value, n_split)
            n_samples_lX, _ = lX.shape
            n_samples_rX, _ = rX.shape
            if n_samples_lX > 0:
                iterate(lX, count+1)
            if n_samples_rX >0:
                iterate(rX, count+1)

    if add_index:
        n_samples, _ = X.shape
        X = np.c_[X, range(n_samples)]

    iterate(X)
    return n_split


class iForestp():
    ''' Class to construct the isolation forest.

    -n_estimators: is the number of trees in the forest,

    -sample_size: is the bootstrap parameter used during the construction
    of the forest,

    -add_index: adds a column of index to the matrix X. This is required and 
    add_index can be set to False only if the last column of X contains 
    already indices.

    -max_depth: is the maximum depth of each tree
    '''
    def __init__(self, n_estimators=20, sample_size=None, add_index = True, 
                 max_depth = 100):
        self.n_estimators = n_estimators
        self.sample_size = sample_size
        self.add_index = add_index
        self.max_depth = max_depth
        return

    def fit(self, X):
        print "Entering iForest.fit() ..."
        n_samples, n_features = X.shape
        if self.sample_size == None:
            self.sample_size = int(n_samples/2)

        if self.add_index:
            X = np.c_[X, range(n_samples)]


        trees = [iTree(X[np.random.choice(n_samples, 
                                          self.sample_size, 
                                          replace=False)],
                       max_depth=self.max_depth) 
                 for i in range(self.n_estimators)]

#         trees = []
#         for i in range(self.n_estimators):
#             print ".",
#             iTree(X[np.random.choice(n_samples,
#                                      self.sample_size, 
#                                      replace=False)],
#                   max_depth=self.max_depth) 

        print ""

        self.path_length_ = {k:None for k in range(n_samples)}
        for k in self.path_length_.keys():
            self.path_length_[k] = np.array([tree[k] 
                                             for tree in trees 
                                             if k in tree])
        self.path_length_ = np.array([self.path_length_[k].mean() for k in 
                                      self.path_length_.keys()])
        self.anomaly_score_ = _anomaly_score(self.path_length_, self.sample_size)
        return self

if __name__ == '__main__':

    import time
    start = time.time()
    WORKING_DIR = '/home/wilber/work/Galvanize/gcp-data/iForest/resize'
    data = gl.image_analysis.load_images(WORKING_DIR, random_order=True)

    time1 = time.time()
    print time1 - start
    data['image'] = gl.image_analysis.resize(data['image'], 256, 256)
    totsecs = time.time() - start
    hours = int(totsecs/3600)
    mins = int((totsecs - 3600.*hours)/60)
    secs = totsecs - 3600.*hours - 60.*mins

    ETstrang = "Elapsed time = {0} hours, {1} minutes, {2} seconds"
    print ETstrang.format(hours, mins, secs)

    start = time.time()
    pretrained_model = gl.load_model('http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45')
    time1 = time.time()
    print time1 - start
    data['extracted_features'] = pretrained_model.extract_features(data, 21)
    totsecs = time.time() - start
    print totsecs
    hours = int(totsecs/3600)
    mins = int((totsecs - 3600.*hours)/60)
    secs = totsecs - 3600.*hours - 60.*mins

    print ETstrang.format(hours, mins, secs)

    X21 = np.array(data['extracted_features'])
    print np.shape(X21)
    X21[:5]

    m21 = isof.iForest(n_estimators=10, max_depth=20)
    start = time.time()
    m21.fit(X21)
    end = time.time()
    print "elapsed time: {0}s.".format(end - start)

    anom_scores = m22.anomaly_score_
    sort_indices = np.argsort(anom_scores)

    for i in range(1,16):
        ind = sort_indices[-i]
        print "{0}\t{1}\t{2}".format(i, names[ind], anom_scores[ind])

    print "\n"
    for i in range(1, 2000):
        ind = sort_indices[-i]
        if names[ind].startswith('fig'):
            print "{0}\t{1}\t{2}".format(i, names[ind], anom_scores[ind])
