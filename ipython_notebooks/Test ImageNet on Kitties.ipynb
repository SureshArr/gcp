{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level feature extraction from images using Dato's Graphlab-Create\n",
    "\n",
    "*Unfortunately, while this worked initially, an update of the software led to licencing problems. The short time frame for the capstone project led me to use Caffe as an alternate for the Deep Convolution Neural Network model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A newer version of GraphLab Create (v1.6.1) is available! Your current version is v1.5.2.\n",
      "New features in 1.6:\n",
      "- Time Series data type\n",
      "- Model tuning in Canvas\n",
      "- Churn prediction toolkit\n",
      "- Product sentiment analysis toolkit\n",
      "- DBSCAN for clustering toolkit\n",
      "- Record linker for data matching toolkit\n",
      "- Frequent pattern mining toolkit\n",
      "- Support adaptive Predictive Services model serving through endpoint policies\n",
      "- Distributed Machine Learning in EC2\n",
      "- Interface between DataFrames and SFrames in scala\n",
      "\n",
      "Notable performance improvements:\n",
      "- Improve service latency for all supervised learning models\n",
      "- Improved performance of nearest neighbor toolkit by constructing a similarity graph directly\n",
      "- Fast approximation of nearest neighbors through locality-sensitive hashing\n",
      "- More efficient and faster access of data in S3\n",
      "- Improved performance of distributed graph analytics\n",
      "\n",
      "For detailed release notes please visit:\n",
      "https://dato.com/download/release-notes.html\n",
      "\n",
      "-\n",
      "You can use pip to upgrade the graphlab-create package. For more information see https://dato.com/products/create/upgrade.\n",
      "[WARNING] This Python session does not appear to be running in an interactive IPython Notebook. Use of the 'ipynb' target may behave unexpectedly or result in errors.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "import matplotlib.pyplot as plt\n",
    "import graphlab as gl\n",
    "import os\n",
    "import glob\n",
    "\n",
    "gl.canvas.set_target('ipynb')\n",
    "#from code import iForest as isof\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now let's have a look at cat photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This non-commercial license of GraphLab Create is assigned to wilber@ssl.berkeley.eduand will expire on September 15, 2016. For commercial licensing options, visit https://dato.com/buy/.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-3860 - Server binary: /usr/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1443230189.log\n",
      "[INFO] GraphLab Server Version: 1.5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62452983856\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Elapsed time = 0 hours, 0 minutes, 5.573969841 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "WORKING_DIR = '/home/wilber/work/Galvanize/gcp-data/iForest/resize'\n",
    "data = gl.image_analysis.load_images(WORKING_DIR, \\\n",
    "                                     random_order=True)\n",
    "time1 = time.time()\n",
    "print time1 - start\n",
    "data['image'] = gl.image_analysis.resize(data['image'], 256, 256)\n",
    "totsecs = time.time() - start\n",
    "hours = int(totsecs/3600)\n",
    "mins = int((totsecs - 3600.*hours)/60)\n",
    "secs = totsecs - 3600.*hours - 60.*mins\n",
    "print \"Elapsed time = {0} hours, {1} minutes, {2} seconds\".format(hours, mins, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['House_Cat_hd_Wallpaper.png',\n",
       " '6974484-cute-cat-smile.png',\n",
       " 'cat-pictures-of-house-cats-122008-spooky.png',\n",
       " 'maxresdefault.png',\n",
       " 'dbe5f0727b69487016ffd67a6689e75a.png',\n",
       " 'Cat3.png',\n",
       " '99059361-choose-cat-litter-632x475.png',\n",
       " '5705f938c04ca27c38f24fc9e7a2f68b.png',\n",
       " 'kitten_250.png',\n",
       " 'willoween1.png']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = data['path']\n",
    "names = map(lambda x: x.split('/')[-1], names)\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Fetch Dato's ImageNet-trained deep neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Downloading http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45/dir_archive.ini to /var/tmp/graphlab-wilber/3860/000000.ini\n",
      "PROGRESS: Downloading http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45/objects.bin to /var/tmp/graphlab-wilber/3860/000001.bin\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45 is not a valid file name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-500a33e67bf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpretrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtime1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtime1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/graphlab/toolkits/_model.pyc\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# Not a ToolkitError so try unpickling the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGLUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m# Get the version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/graphlab/_gl_pickle.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s is not a valid file name.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;31m# GLC 1.3 Pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45 is not a valid file name."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pretrained_model = gl.load_model('http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45')\n",
    "time1 = time.time()\n",
    "print time1 - start\n",
    "\n",
    "pretrained_model.save('ImageNet45')\n",
    "test = model_load('ImageNet45')\n",
    "time2 = time.time()\n",
    "print time2 - time1, time2 - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hope to save the model for later restore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pretrained_model.save('ImageNet45')\n",
    "\n",
    "test = model_load('ImageNet45')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['extracted_features'] = pretrained_model.extracted_features(data, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e5d9f382e688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'extracted_features'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtotsecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtotsecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotsecs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotsecs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m3600.\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mhours\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pretrained_model' is not defined"
     ]
    }
   ],
   "source": [
    "data['extracted_features'] = pretrained_model.extract_features(data, 21)\n",
    "totsecs = time.time() - start\n",
    "print totsecs\n",
    "hours = int(totsecs/3600)\n",
    "mins = int((totsecs - 3600.*hours)/60)\n",
    "secs = totsecs - 3600.*hours - 60.*mins\n",
    "print \"Elapsed time = {0} hours, {1} minutes, {2} seconds\".format(hours, mins, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X21 = np.array(data['extracted_features'])\n",
    "print np.shape(X21)\n",
    "X21[:5]\n",
    "\n",
    "m21 = isof.iForest(n_estimators=50, max_depth=100)\n",
    "start = time.time()\n",
    "m21.fit(X21)\n",
    "end = time.time()\n",
    "print \"elapsed time: {0}s.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This non-commercial license of GraphLab Create is assigned to wilber@ssl.berkeley.eduand will expire on September 15, 2016. For commercial licensing options, visit https://dato.com/buy/.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-11450 - Server binary: /usr/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1442961464.log\n",
      "[INFO] GraphLab Server Version: 1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.59461402893\n",
      "Couldn't import dot_parser, loading of dot files will not be possible."
     ]
    }
   ],
   "source": [
    "anom_scores = m22.anomaly_score_\n",
    "sort_indices = np.argsort(anom_scores)\n",
    "\n",
    "for i in range(1,16):\n",
    "    ind = sort_indices[-i]\n",
    "    print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])\n",
    "\n",
    "print \"\\n\"\n",
    "for i in range(1, 2000):\n",
    "    ind = sort_indices[-i]\n",
    "    if names[ind].startswith('fig'):\n",
    "        print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try iForest using real image data (raw pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level features from Dato's ImageNet classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "WORKING_DIR = '/home/wilber/work/Galvanize/gcp-data/iForest/loonie'\n",
    "data = gl.image_analysis.load_images(WORKING_DIR, \\\n",
    "                                     random_order=True)\n",
    "time1 = time.time()\n",
    "print time1 - start\n",
    "data['image'] = gl.image_analysis.resize(data['image'], 256, 256)\n",
    "totsecs = time.time() - start\n",
    "hours = int(totsecs/3600)\n",
    "mins = int((totsecs - 3600.*hours)/60)\n",
    "secs = totsecs - 3600.*hours - 60.*mins\n",
    "print \"Elapsed time = {0} hours, {1} minutes, {2} seconds\".format(hours, mins, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = data['path']\n",
    "names = map(lambda x: x.split('/')[-1], names)\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data from MNIST\n",
    "#data = gl.SFrame('http://s3.amazonaws.com/dato-datasets/mnist/sframe/train6k')\n",
    "\n",
    "# Create a DeepFeatureExtractorObject\n",
    "\n",
    "start = time.time()\n",
    "extractor = gl.feature_engineering.DeepFeatureExtractor(feature = 'image')\n",
    "print help(extractor)\n",
    "# Fit the encoder for a given dataset.\n",
    "time1 = time.time()\n",
    "print \"\\n\\n\", time1 - start\n",
    "extractor = extractor.fit(data)\n",
    "time2 = time.time()\n",
    "print \"\\n\\n\", time2 - time1\n",
    "\n",
    "# Return the model used for the deep feature extraction.\n",
    "extracted_model = extractor['model']\n",
    "time3 = time.time()\n",
    "print \"\\n\\n\", time3 - time2\n",
    "\n",
    "# Extract features.\n",
    "features_sf = extractor.transform(data)\n",
    "features_sf.head()\n",
    "time4 = time.time()\n",
    "print \"\\n\\n\", time4 - time3, \", total time: \", time4 - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature = features_sf.head['deep_features_image']\n",
    "features_sf.num_rows(), features_sf.num_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opts_dict = extractor.get_current_options()\n",
    "for key, value in opts_dict.iteritems():\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repr(extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_sf.column_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(features_sf['deep_features_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(features_sf['deep_features_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = isof.iForest(n_estimators=200)\n",
    "start = time.time()\n",
    "model.fit(X)\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anom_scores = model.anomaly_score_\n",
    "sort_indices = np.argsort(anom_scores)\n",
    "\n",
    "for i in range(1,16):\n",
    "    ind = sort_indices[-i]\n",
    "    print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])\n",
    "\n",
    "print \"\\n\"\n",
    "for i in range(1, 2000):\n",
    "    ind = sort_indices[-i]\n",
    "    if names[ind].startswith('fig'):\n",
    "        print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different layers from pre-trained model:\n",
    "\n",
    "#### Layer 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Downloading http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45/dir_archive.ini to /var/tmp/graphlab-wilber/29977/862a9888-7e64-422c-b70d-660e1c324f60.ini\n",
      "PROGRESS: Downloading http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45/objects.bin to /var/tmp/graphlab-wilber/29977/573cd2a3-9ce1-4f25-a5bb-5a270497b1d7.bin\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45 is not a valid file name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b2742f3390eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpretrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtime1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#pretrained_model = gl.load_model('/home/wilber/work/Galvanize/gcp/DatoImageNetIter45.bin')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/graphlab/toolkits/_model.pyc\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m# Not a ToolkitError so try unpickling the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGLUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# Get the version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/graphlab/_gl_pickle.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s is not a valid file name.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;31m# GLC 1.3 Pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45 is not a valid file name."
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "start = time.time()\n",
    "pretrained_model = gl.load_model('http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45')\n",
    "time1 = time.time()\n",
    "#pretrained_model = gl.load_model('/home/wilber/work/Galvanize/gcp/DatoImageNetIter45.bin')\n",
    "print \"dt1: \", time1 - start\n",
    "data['extracted_features'] = pretrained_model.extract_features(data, 22)\n",
    "print time.time() - time1\n",
    "totsecs = time.time() - start\n",
    "hours = int(totsecs/3600)\n",
    "mins = int((totsecs - 3600.*hours)/60)\n",
    "secs = totsecs - 3600.*hours - 60.*mins\n",
    "print \"Elapsed time = {0} hours, {1} minutes, {2} seconds\".format(hours, mins, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretrained_model.save('/home/wilber/work/Galvanize/gcp/DatoImageNetIter45.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X22 = np.array(data['extracted_features'])\n",
    "print np.shape(X22)\n",
    "X22[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m22 = isof.iForest(n_estimators=200)\n",
    "start = time.time()\n",
    "m22.fit(X22)\n",
    "end = time.time()\n",
    "print \"elapsed time: {0}s.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anom_scores = m22.anomaly_score_\n",
    "sort_indices = np.argsort(anom_scores)\n",
    "\n",
    "for i in range(1,16):\n",
    "    ind = sort_indices[-i]\n",
    "    print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])\n",
    "\n",
    "print \"\\n\"\n",
    "for i in range(1, 2000):\n",
    "    ind = sort_indices[-i]\n",
    "    if names[ind].startswith('fig'):\n",
    "        print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#pretrained_model = gl.load_model('http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45')\n",
    "#time1 = time.time()\n",
    "#print time1 - start\n",
    "data['extracted_features'] = pretrained_model.extract_features(data, 21)\n",
    "totsecs = time.time() - start\n",
    "print totsecs\n",
    "hours = int(totsecs/3600)\n",
    "mins = int((totsecs - 3600.*hours)/60)\n",
    "secs = totsecs - 3600.*hours - 60.*mins\n",
    "print \"Elapsed time = {0} hours, {1} minutes, {2} seconds\".format(hours, mins, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X21 = np.array(data['extracted_features'])\n",
    "print np.shape(X21)\n",
    "X21[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m21 = isof.iForest(n_estimators=200)\n",
    "start = time.time()\n",
    "m21.fit(X21)\n",
    "end = time.time()\n",
    "print \"elapsed time: {0}s.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anom_scores = m21.anomaly_score_\n",
    "sort_indices = np.argsort(anom_scores)\n",
    "\n",
    "for i in range(1,16):\n",
    "    ind = sort_indices[-i]\n",
    "    print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])\n",
    "\n",
    "print \"\\n\"\n",
    "for i in range(1, 2000):\n",
    "    ind = sort_indices[-i]\n",
    "    if names[ind].startswith('fig'):\n",
    "        print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# pretrained_model = gl.load_model('http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45')\n",
    "# time1 = time.time()\n",
    "# print time1 - start\n",
    "data['extracted_features'] = pretrained_model.extract_features(data, 20)\n",
    "print time.time() - time1\n",
    "totsecs = time.time() - start\n",
    "hours = int(totsecs/3600)\n",
    "mins = int((totsecs - 3600.*hours)/60)\n",
    "secs = totsecs - 3600.*hours - 60.*mins\n",
    "print \"Elapsed time = {0} hours, {1} minutes, {2} seconds\".format(hours, mins, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X20 = np.array(data['extracted_features'])\n",
    "print np.shape(X20)\n",
    "X20[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m20 = isof.iForest(n_estimators=200)\n",
    "start = time.time()\n",
    "m20.fit(X20)\n",
    "end = time.time()\n",
    "print \"elapsed time: {0}s.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anom_scores = m20.anomaly_score_\n",
    "sort_indices = np.argsort(anom_scores)\n",
    "\n",
    "for i in range(1,16):\n",
    "    ind = sort_indices[-i]\n",
    "    print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])\n",
    "\n",
    "print \"\\n\"\n",
    "for i in range(1, 2000):\n",
    "    ind = sort_indices[-i]\n",
    "    if names[ind].startswith('fig'):\n",
    "        print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# pretrained_model = gl.load_model('http://s3.amazonaws.com/dato-datasets/deeplearning/imagenet_model_iter45')\n",
    "# time1 = time.time()\n",
    "# print time1 - start\n",
    "data['extracted_features'] = pretrained_model.extract_features(data, 19)\n",
    "print time.time() - time1\n",
    "totsecs = time.time() - start\n",
    "hours = int(totsecs/3600)\n",
    "mins = int((totsecs - 3600.*hours)/60)\n",
    "secs = totsecs - 3600.*hours - 60.*mins\n",
    "print \"Elapsed time = {0} hours, {1} minutes, {2} seconds\".format(hours, mins, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X19 = np.array(data['extracted_features'])\n",
    "print np.shape(X19)\n",
    "X19[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m19 = isof.iForest(n_estimators=200)\n",
    "start = time.time()\n",
    "m19.fit(X19)\n",
    "end = time.time()\n",
    "print \"elapsed time: {0}s.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anom_scores = m19.anomaly_score_\n",
    "sort_indices = np.argsort(anom_scores)\n",
    "for i in range(1,16):\n",
    "    ind = sort_indices[-i]\n",
    "    print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])\n",
    "\n",
    "print \"\\n\"\n",
    "for i in range(1, 2000):\n",
    "    ind = sort_indices[-i]\n",
    "    if names[ind].startswith('fig'):\n",
    "        print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With layer 19, clearly have gone too far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try with raw pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = isof.iForest(n_estimators=200)\n",
    "start = time.time()\n",
    "model.fit(Xraw)\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anom_scores = model.anomaly_score_\n",
    "sort_indices = np.argsort(anom_scores)\n",
    "\n",
    "for i in range(1,16):\n",
    "    ind = sort_indices[-i]\n",
    "    print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])\n",
    "\n",
    "print \"\\n\"\n",
    "for i in range(1, 2000):\n",
    "    ind = sort_indices[-i]\n",
    "    if names[ind].startswith('fig'):\n",
    "        print \"{0}\\t{1}\\t{2}\".format(i, names[ind], anom_scores[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
