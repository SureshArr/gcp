import graphlab as _gl
import logging as _logging
import subprocess
import tempfile
import os
from copy import copy
from graphlab_util import file_util
from graphlab.util import _make_internal_url
import logging
from _dml_logging import LogPrinter
from _dml_env import DMLRemoteEnvironment
from _dml_env import change_job_logger_level

_log = _logging.getLogger(__name__)

# File indicates commander teminated
COMMANDER_COMPLETE_FILE = 'commander.complete'
COMMANDER_LOG_FILE_PREFIX = 'commander_log'
PROGRESS_LOG_FILE = 'progress.log'
COMMANDER_LOG_SERVER_ADDRESS_FILE = 'log_server_address'


def _sanitize_internal_s3_url(input_str):
    from graphlab.connect.aws import get_credentials
    sanitized_str = input_str
    aws_id, aws_key = get_credentials()
    sanitized_str = sanitized_str.replace(aws_id, '')
    sanitized_str = sanitized_str.replace(aws_key, '')
    return sanitized_str


def dml_exec(function_name, data, env='auto', verbose=True, **kwargs):
    """
    Executes a distributed ml function

    Parameters
    ----------
    function_name : str
        Name of the distributed function to be executed. The function symbol
        must exists in the unity distributed shared library.

    data : dict
        Key value arguments to the function stored in a dictionary

    env : DMLEnvironemnt
        Contains job environment parameters and a job submit function.

    **kwargs : dict
        Additional options.
        See _get_worker_args and _get_commander_args.
            - check_hdfs : {0, 1} Perform sanity check for hdfs read and write
            - startup_timeout : int Timeout in seconds for cluster setup

    Return
    ------
    (success, message, result_path) : bool, str, str
    """
    if env == 'auto':
        env = DMLRemoteEnvironment()

    if not file_util.exists(env.working_dir):
        _log.debug('Creating working directory: %s' % env.working_dir)
        file_util.mkdir(env.working_dir)
    else:
        _log.debug('Using existing working directory: %s' % env.working_dir)

    # Job function arguments
    try:
        args = _gl.extensions.dml_function_invocation()
        data_copy = copy(data)
        internal_working_dir = _make_internal_url(env.working_dir)
        data_copy['__base_path__'] = internal_working_dir
        args.from_dict(data_copy, internal_working_dir)
        json_data = args.to_str()

        # sanitize the base path url
        sanitized_json_data = _sanitize_internal_s3_url(json_data)
        _log.info('Serialized arguments: %s' % sanitized_json_data)
    except Exception as e:
        success = False
        message = 'Error serializing arguments. %s' % str(e)
        return (success, message, None)

    # Submit job
    try:
        job = dml_submit(function_name, json_data, env,
                         metric_server_address_file=COMMANDER_LOG_SERVER_ADDRESS_FILE,
                         logprogress_file=PROGRESS_LOG_FILE,
                         **kwargs)
    except KeyboardInterrupt:
        message = 'Canceled by user'
        return (success, message, None)

    if verbose:
        log_server_address_path = os.path.join(env.working_dir,
                                               COMMANDER_LOG_SERVER_ADDRESS_FILE)
        log_file_path = os.path.join(env.working_dir,
                                     PROGRESS_LOG_FILE)

        logprinter = LogPrinter(log_server_address_path,
                                log_file_path)
        logprinter.start()

    _log.debug('Wait for job to finish')
    (success, message) = _wait_and_parse_job_result(job)
    if (message is not None):
        message += '\nFor more details please check logs under %s' % env.working_dir

    if verbose:
        logprinter.stop()

    if success:
        try:
            result_path = os.path.join(env.working_dir, env.output_name)
            ret_str = file_util.read(result_path)
            sanitized_ret_str = _sanitize_internal_s3_url(ret_str)
            _log.info('Deserializing results: %s' % sanitized_ret_str)

            args.from_str(ret_str)
            response = args.to_dict()

            # Check toolkit response for "result" key or "exception" key.
            if 'result' in response:
                return (success, message, response['result'])
            elif 'exception' in response:
                return (False, response['exception'], None)
            else:
                raise ValueError('Invalid toolkit response. Must have "result" or \
                                 "exception" as key')
        except Exception as e:
            success = False
            message = 'Error deserializing results. %s' % str(e)
            return (success, message, None)
    else:
        return (success, message, None)

    #We leave the directory cleanup to the cluster manager


def dml_submit(function_name, str_data, env, **kwargs):
    """
    Executes a distributed ml function

    Parameters
    ----------
    function_name : str
        Name of the distributed function to be executed. The function symbol
        must exists in the unity distributed shared library.

    str_data : str
        Arguments as serialized string to be passed to the distributed
        function.

    env : DMLEnvironemnt
        Contains job environment parameters and a job submit function.

    **kwargs : dict
        Additional options. See _get_worker_args and _get_commander_args.
        - check_hdfs : {0, 1} Perform sanity check for hdfs read and write
        - startup_timeout : int Timeout in seconds for cluster setup

    Return
    ------
    job : map_job
    """
    _log.debug('Submitting job')
    map_job_args = _get_dml_exec_args(function_name, str_data, env,
                                      output_name=env.output_name,
                                      **kwargs)

    _log.debug('job arguments: %s' % str(map_job_args))
    job = env.submit(subprocess_exe, map_job_args)
    return job


# /**************************************************************************/
# /*                                                                        */
# /*                            Helper function                             */
# /*                                                                        */
# /**************************************************************************/
def subprocess_exe(exe, args, setup=None, teardown=None, out_log_prefix=None):
    """
    Wrapper function to execute an external program.
    This function is exception safe, and always catches
    the error.

    Parameters
    ----------
    exe : str
        The command to run
    args : list[str]
        Arguments to passed to the command
    setup : function
        Setup function to run before executing the command
    teardown : function
        Teardown function to run after executing the command
    out_log_prefix: str
        The path prefix to the saved log file.
        If set, the logs will be save to the following locations:
            <prefix>.stdout
            <prefix>.stderr
        and the return value will contain paths to the log files.
        The path can be local or hdfs or s3.

    Return
    ------
    out : dict
        A dictionary containing the following keys:

        success : bool
            True if the command succeeded
        return_code : int
            The return code of the command
        stderr : str
            Path to the stderr log of the process
        stdout : str
            Path to the stdout log of the process
        python_exception : Exception
            Python exception
    """
    import logging
    import os
    ret = {'success': True,
           'return_code': None,
           'stdout': None,
           'stderr': None,
           'python_exception': None}

    # Creates local log files
    try:
        local_log_stdout = tempfile.NamedTemporaryFile(delete=False)
        local_log_stderr = tempfile.NamedTemporaryFile(delete=False)
    except Exception as e:
        ret['success'] = False
        ret['python_exception'] = e

   # Run setup
    try:
        if setup is not None:
            setup()
    except Exception as e:
        ret['success'] = False
        ret['python_exception'] = e

   # Executes the command
    if ret['success']:
        try:
            proc = subprocess.Popen([exe] + args,
                                    stdout=local_log_stdout,
                                    stderr=local_log_stderr)
            proc.communicate()
            ret['success'] = proc.returncode == 0
            ret['return_code'] = proc.returncode
        except Exception as e:
            ret['success'] = False
            ret['python_exception'] = e
        finally:
            try:
                local_log_stdout.close()
                local_log_stderr.close()
                if out_log_prefix is not None:
                    # persistent hdfs files for logging. When local log closed,
                    # they will be loaded to the corresponding hdfs path
                    file_log_stdout = out_log_prefix + '.stdout'
                    file_log_stderr = out_log_prefix + '.stderr'
                    # copy to target log path
                    file_util.copy_from_local(local_log_stdout.name, file_log_stdout)
                    file_util.copy_from_local(local_log_stderr.name, file_log_stderr)
                    ret['stdout'] = file_log_stdout
                    ret['stderr'] = file_log_stderr
                else:
                    ret['stdout'] = open(local_log_stdout.name).read()
                    ret['stderr'] = open(local_log_stderr.name).read()

                os.remove(local_log_stdout.name)
                os.remove(local_log_stderr.name)
            except Exception as e:
                ret['_save_log_exception'] = e
                logging.warn(str(e))

    # Teardown
    if teardown is not None:
        try:
            teardown()
        except Exception as e:
            ret['_tear_down_exception'] = e
            logging.warn(str(e))

    return ret


@change_job_logger_level(logging.WARNING)
def _wait_and_parse_job_result(job):
    """
    Wait for job to finsh, and check the job result

    Return
    ------
    (success, message) : bool, str

      If all job succeeded, return (True, None)
      Exception can happen in the following settings
      - job.get_results() failed
      - job.get_results() succeeded, and the commander result failed
      - job.get_results() succeeded, and worker results failed
    """
    job_results = None
    success = False
    message = None

    try:
        job_results = job.get_results()
    except KeyboardInterrupt:
        try:
            job.cancel()
        except:
            pass
        message = 'Canceled by user'
        return (success, message)
    except:
        message = 'Unable to get job results. Exception message: '
        metrics = job.get_metrics()
        if metrics is not None:
            message += job.get_metrics()['exception_message']
        else:
            message += 'None'
        return (success, message)

    success = all(x['success'] for x in job_results)

    if success:
        return (success, message)

    # Here job failed. We collect the python exception messages
    # from commander and workers.
    error_messages = []
    for i in range(len(job_results)):
        if not job_results[i]['success']:
            msg = ''
            if i == 0:
                msg += 'Commander error: '
            else:
                msg += 'Worker %d error: ' % i
            e = job_results[i]['python_exception']
            if e is not None:
                msg += str(e)
            else:
                msg += "Unknown"
            error_messages.append(msg)
    message = '\n'.join(error_messages)
    return (success, message)


def _get_commander_args(function_name, data, env,
                        cluster_type='standalone_passive',
                        output_name='out',
                        **kwargs):
    args = dict()
    # from arguments
    args['function'] = function_name
    args['args'] = data
    args['num_nodes'] = env.num_workers
    args['startup_timeout'] = 30
    args['working_dir'] = _make_internal_url(env.working_dir)

    # from optional arguments
    args['shared_lib'] = env.LIB_UNITY_DISTRIBUTED_PATH
    args['cluster_type'] = cluster_type
    args['output_name'] = output_name

    # from kwargs, could overwrite existing args
    accepted_args = args.keys() + ['check_hdfs', 'startup_timeout',
                                   'metric_server_address_file',
                                   'metric_server_port']
    for key in accepted_args:
        if key in kwargs:
            args[key] = kwargs[key]

    # return a formated list
    return ['--%s=%s' % (k, v) for k, v in args.iteritems()]


def _get_commander_setup(working_dir):
    """
    Return commander's setup function

    The setup function creates the working directory
    """
    def setup_fun():
        if not file_util.exists(working_dir):
            file_util.mkdir(working_dir)
    return setup_fun


def _get_commander_teardown(working_dir):
    """
    Return commander's teardown function

    The teardown function writes an empty file
    "commander.complete" to the working directory
    """
    def teardown_fun():
        commander_state_file = working_dir + '/' + COMMANDER_COMPLETE_FILE
        file_util.touch(commander_state_file)
    return teardown_fun


def _get_worker_args(worker_id, working_dir, **kwargs):
    args = dict()
    args['worker_id'] = worker_id
    args['working_dir'] = working_dir
    args['startup_timeout'] = 30

    accepted_args = ['check_hdfs', 'startup_timeout', 'consensus_address'] + args.keys()
    for key in accepted_args:
        if key in kwargs:
            args[key] = kwargs[key]
    return ['--%s=%s' % (k, v) for k, v in args.iteritems()]


def _get_dml_exec_args(function_name, data, env, **kwargs):
    """
    Return a list of map job arguments for distributed exec
    """
    internal_working_dir = _make_internal_url(env.working_dir)
    map_job_args = [{'exe': env.COMMANDER_PATH,
                     'args': _get_commander_args(function_name, data, env, **kwargs),
                     'setup': _get_commander_setup(env.working_dir),
                     'teardown': _get_commander_teardown(env.working_dir),
                     'out_log_prefix': env.working_dir + '/' + COMMANDER_LOG_FILE_PREFIX
                     }]

    for i in range(env.num_workers):
        worker_args = {'exe': env.WORKER_PATH,
                       'args': _get_worker_args(i, internal_working_dir, **kwargs),
                       'out_log_prefix': env.working_dir + '/worker_%d_log' % i}
        map_job_args.append(worker_args)
    return map_job_args
