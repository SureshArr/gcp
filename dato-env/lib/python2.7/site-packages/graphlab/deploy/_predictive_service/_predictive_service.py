from ConfigParser import ConfigParser as _ConfigParser
from copy import copy as _copy
from logging import getLogger as _getLogger
from re import compile as _compile
from requests import ConnectionError as _ConnectionError
from uuid import uuid4 as _random_uuid

import datetime as _datetime
import dateutil as _dateutil
import json as _json
import os as _os
import time as _time
import types as _types
import functools as _functools
import re as _re
import shutil as _shutil
import inspect as _inspect

from boto import connect_s3 as _connect_s3
from boto.s3.key import Key as _s3_key

from graphlab_util.file_util import is_path as _is_path, parse_s3_path as _parse_s3_path,  \
    s3_recursive_delete as _s3_recursive_delete, s3_delete_key as _s3_delete_key, \
    retry as _retry, is_s3_path as _is_s3_path, is_local_path as _is_local_path

import graphlab as _gl
import graphlab.connect as _mt
import graphlab.canvas as _canvas
from graphlab.util import _dt_to_utc_timestamp, _utc
from graphlab.deploy._artifact import Artifact as _Artifact
from . import PREDICTIVE_SERVICE_SCHEMA_VERSION, INTERNAL_PREDICTIVE_OBJECT_NAMES, \
    LAST_SUPPORTED_PREDICTIVE_SERVICE_SCHEMA_VERSION

NODE_LAUNCH_LIMIT = 5

from _system_config import SystemConfig as _SystemConfig
from _predictive_service_environment import PredictiveServiceEnvironment
from _predictive_service_environment import DockerPredictiveServiceEnvironment as _DockerPredictiveServiceEnvironment
from _predictive_service_environment import predictive_service_environment_factory
from _predictive_service_environment import PORT_DEFAULT_NUM as _PORT_DEFAULT_NUM
from _model_predictive_object import ModelPredictiveObject
from _predictive_object import PredictiveObject as _PredictiveObject
from _custom_query_predictive_object import CustomQueryPredictiveObject as _CustomQueryPredictiveObject
from _predictive_client import NonExistError as _NonExistError

_logger = _getLogger(__name__)
_name_checker = _compile('^[a-zA-Z-]+$')

def _is_cache_enabled(status):
    return status.get("cache_state", "enabled") == "enabled"

def _check_PO_name(name):
    if ":" in name or "=" in name:
        raise ValueError("\":\" or \"=\" characters cannot be present in the Predictive Object name")
    if name in INTERNAL_PREDICTIVE_OBJECT_NAMES:
        raise ValueError("Cannot name Predictive Object to \"%s\"" % name)

_VALID_CONFIG_PARAMS = {
    "cache_max_memory_mb": (lambda v: True if isinstance(v, int) and v > 0 else False),
    "cache_ttl_on_update_secs": (lambda v: True if isinstance(v, int) and v >= 0 else False)
}

def _config_params_are_valid(params):
    for param, value in params.items():
        if param not in _VALID_CONFIG_PARAMS:
            _logger.error("Invalid configuration parameter: %s" % param)
            return False
        if not _VALID_CONFIG_PARAMS[param](value):
            _logger.error("Invalid value for configuration parameter %s: %s" \
                          % (param, str(value)))
            return False
    return True

def _models_unavailable(status_sf):
    if status_sf['models'].dtype() not in (list, dict):
        msg = 'Status unavailable; '

        if "OutOfService" in {x for x in status_sf["state"].unique()}:
            msg += "nodes are unavailable"
        else:
            msg += "no models deployed in any instance"

        return msg

class PredictiveService(_Artifact):
    """
    Monitor/Manage a running Predictive Service.

    Predictive Service objects should not be instantiated directly, and are
    intended to be created using :func:`graphlab.deploy.predictive_service.create`
    or loaded using :func:`graphlab.deploy.predictive_service.load`.

    See Also
    --------
    graphlab.deploy.predictive_service.create, graphlab.deploy.predictive_service.load

    """

    # State File Config Section Names
    _DEPLOYMENT_SECTION_NAME = 'Predictive Objects Service Versions'
    _PREDICTIVE_OBJECT_DOCSTRING = 'Predictive Objects Docstrings'
    _ENVIRONMENT_SECTION_NAME = 'Environment Info'
    _SERVICE_INFO_SECTION_NAME = 'Service Info'
    _SYSTEM_SECTION_NAME = 'System'
    _META_SECTION_NAME = 'Meta'

    # Directory Names
    _PREDICTIVE_OBJECT_DIR = 'predictive_objects'
    _DEPENDENCIES_DIR ='dependencies'

    _typename = 'PredictiveService'

    def __repr__(self):
        return self.__str__()

    def __str__(self):
        if not self._environment:
            return "Predictive Service '%s' at '%s' has been terminated." % (self.name, self._state_path)

        ret = ""
        ret += 'Name                  : %s' % self.name + '\n'
        ret += 'State Path            : %s' % self._state_path + '\n'
        ret += 'Description           : %s' % self.description + '\n'
        ret += 'API Key               : %s' % self.api_key + '\n'
        ret += 'CORS origin           : %s' % self._cors_origin + '\n'
        ret += 'Global Cache State    : %s' % self._global_cache_state + '\n'
        if self._environment is not None:
            ret += 'Load Balancer DNS Name: %s' % self._environment.load_balancer_dns_name + '\n'

        ret += "\nDeployed predictive objects:\n"
        for (po_name, po_info) in self._predictive_objects.iteritems():
            ret += '\tname: %s, version: %s, cache: %s, description: %s\n' % (po_name, po_info['version'], po_info['cache_state'], po_info['description'])

        if len(self._local_changes) == 0:
            ret += "No Pending changes.\n"
        else:
            ret += "Pending changes: \n"
            for (po_name, po_info) in self._local_changes.iteritems():
                version = po_info[1]['version'] if po_info[1] else None
                if version:
                    if version == 1:
                        ret += '\tAdding: %s, description: %s\n' % (po_name, po_info[1]['description'])
                    else:
                        ret += '\tUpdating: %s to version: %s, description: %s\n' % (po_name, version, po_info[1]['description'])
                else:
                    ret += "\tRemoving: %s\n" % po_name

        return ret

    @property
    def _all_predictive_objects(self):
        '''
        Return all predictive objects that are currently registered with a
        Predictive Service, including both deployed and pending objects.

        Returns
        --------
        out : a dictionary with predictive object names as keys and the current
        state of the predictive object as the correspond value.

        Examples
        --------

        >>> ps.predictive_objects
        {'recommender_one': {'version': 1, 'state': 'deployed', 'docstring':'this is my first recommender'},
         'recommender_two': {'version': 2, 'state': 'pending', 'docstring':'this is my second recommender'},
        '''
        ret = {}
        for (po_name, po_info) in self._predictive_objects.iteritems():
            ret[po_name] = {'state': 'deployed',
                            'version': po_info['version'],
                            'description': po_info['description'],
                            'schema_version': po_info['schema_version'],
                            'cache_state': po_info['cache_state'],
                            'docstring': po_info['docstring']}

        # Add local changes on top of deployed ones
        for po_name in self._local_changes:
            (po_obj, po_info) = self._local_changes[po_name]
            if po_obj is not None:
                ret[po_name] = {'state': 'pending',
                                'version':po_info['version'],
                                'description': po_obj.description,
                                'schema_version': po_obj.schema_version,
                                'cache_state': po_info['cache_state'],
                                'docstring': po_info['docstring']}
            else:  # delete
                del ret[po_name]

        return ret

    @property
    def deployed_predictive_objects(self):
        '''
        Return all deployed Predictive Objects for a Predictive Service.

        Returns
        --------
        out : dict
            One entry for each Predictive Object. The keys are the names of the
            objects and the values are the corresponding versions.

        Examples
        --------

        >>> ps.deployed_predictive_objects
            {'recommender_one': 2, 'recommender_two': 1}
        '''
        self._ensure_not_terminated()

        ret = {}
        for key, value in self._predictive_objects.iteritems():
            ret[key] = {
                'version': value['version'],
                'description': value['description'],
                'cache_state': value['cache_state']
            }

        return ret

    @property
    def pending_changes(self):
        '''
        Return all currently pending updates for a Predictive Service.

        Returns
        --------
        out : dict
            Keys are the changed Predictive Object names, values are a
            dictionary with two keys: 'action' and 'version'. 'action' is one of
            'add', 'update', or 'remove' and 'version' is the new version
            number or None if action is 'remove'

        Examples
        --------

        >>> ps.pending_changes
        Out:
            {'recommender_one': {'action': 'remove', 'version': None},
             'recommender_three': {'action': 'add', 'version': 1},
             'recommender_two': {'action': 'update', 'version': 2}}
        '''
        ret = {}
        for po_name in self._local_changes:
            (po_obj, po_info) = self._local_changes[po_name]
            po_version = po_info['version'] if po_info is not None else None
            po_cache_state = po_info['cache_state'] if po_info is not None else None
            if po_version == 1:
                ret[po_name] = {'action': 'add', 'version':po_version, 'cache_state': po_cache_state}
            elif po_obj == None:  # deleted case
                ret[po_name] = {'action': 'remove', 'version':None}
            else:
                ret[po_name] = {'action': 'update', 'version':po_version, 'cache_state': po_cache_state}
        return ret

    def __init__(self, name, state_path, description, api_key, admin_key,
                 aws_credentials,
                 _new_service=True, cors_origin='',
                 global_cache_state='enabled', system_config=None,
                 port = _PORT_DEFAULT_NUM):
        '''
        Initialize a new Predictive Service object
        Notes
        -----
        Do not call this method directly.
        To create a new Predictive Service, use:
             graphlab.deploy.predictive_service.create(...)
        To load an existing Predictive Service, use
            graphlab.deploy.predictive_service.load(<ps-state-path>)
        '''
        if type(name) != str:
            raise TypeError("Name of Predictive Service needs to be a string")

        self.name = name
        self.description = description
        self.api_key = api_key
        self.admin_key = admin_key
        self.port = port
        if (port < 0 or port > 65535):
            raise ValueError("Port must be within a certain range, 0 to 65535.")
        self._cors_origin = cors_origin
        self._global_cache_state = global_cache_state
        self._system_config = system_config or _SystemConfig()
        self.aws_credentials = aws_credentials

        self._local_changes = {}
        self._predictive_objects = {}
        self._state_path = state_path
        self._session = _gl.deploy._default_session

        if _new_service:
            # Init version data
            self._revision_number = 0
            self._schema_version = PREDICTIVE_SERVICE_SCHEMA_VERSION

            # No environment yet. A launched one must be attached later.
            self._environment = None

    def describe(self, po_name):
        '''
        Prints doc string for the Predictive Object

        Parameters
        -----------
        po_name : str
            The name of the Predictive Object
        '''
        self._ensure_not_terminated()

        if po_name not in self._predictive_objects.keys():
            raise ValueError("Cannot find a predictive object with name '%s'." % po_name)

        print self._predictive_objects[po_name]['docstring']

    def _get_nodes_status(self):
        '''
        Gets the status of each node in the environment.

        Examples
        --------

        >>> ps._get_nodes_status()
        +--------------------+---------------+-----------+
        |      dns_name      |  instance_id  |   state   |
        +--------------------+---------------+-----------+
        | ec2-54-186-64- ... |  i-00c5a70c   | InService |
        | ec2-54-148-131 ... |  i-01c5a70d   | InService |
        | ec2-54-186-135 ... |  i-03c5a70f   | InService |
        +--------------------+---------------+-----------+
        '''
        self._ensure_not_terminated()

        # get host information
        host_status = []
        for host in self.get_status():
            state = host['state'] if host['state'] == 'InService' else host['state'] + " - " + host['reason']
            if not host.get('cache'):
                cache_state = 'Disabled'
            else:
                cache_state = 'Healthy' if host.get('cache',{}).get('healthy') else 'Unhealthy'
            host_status.append({
                'instance_id':host['id'],
                'dns_name':host['dns_name'],
                'state': state,
                'cache': cache_state})

        if len(host_status) == 0:
            _logger.info('No nodes in the Predictive Service')
            return None
        else:
            return _gl.SArray(data=host_status).unpack('')

    def get_predictive_objects_status(self):
        '''
        Get Predictive Objects deployment status in all nodes.

        Returns
        -------
        out : SFrame
            The status for all deployed Predictive Objects represented
            in a SFrame.

        Examples
        --------

        >>> ps.get_predictive_objects_status()
        +-----------------+------------------+-------------------------+-------------------------+
        |       name      | expected version |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+------------------+-------------------------+-------------------------+
        | Image Predictor |        1         | 1 (Loaded successfully) | 1 (Loaded successfully) |
        | Book Recommender|        2         | 2 (Loaded successfully) | 2 (Loaded successfully) |

        '''
        self._ensure_not_terminated()

        expected = self.deployed_predictive_objects
        result = self.get_status()
        if len(result) == 0:
            _logger.info('No nodes in the Predictive Service')
            return None

        # result is in the following format
        # [{'dns_name': u'ec2-54-191-36-143.us-west-2.compute.amazonaws.com',
        #   'id': u'i-29397b26',
        #   'models': [{u'description': u'None',
        #     u'name': u'Image Predictor',
        #     u'status': u'Loaded successfully',
        #     u'version': 1}],
        #   'reason': u'N/A',
        #   'state': u'InService'},
        #   {...}
        #   ]
        sf = _gl.SArray(result)

        # unpack the dictionary to different columns, result looks like:
        # +------------+------------+--------+-----------+--------------------------------+
        # |  dns_name  |     id     | reason |   state   |             models             |
        # +------------+------------+--------+-----------+--------------------------------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        sf = sf.unpack(column_name_prefix="")

        # in case no model in any instance, there is no way to infer type of 'models' column
        if sf['models'].dtype() == float:
            _logger.info('No models deployed in any instance')
            return

        sf = sf.stack('models', 'models')

        # expand the "models" column, results looks like:
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # |  dns_name  |     id     | reason |   state   | descri ... |    name    |   status   | version |
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService |    None    | Image  ... | Loaded ... |    1    |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | finds  ... | shorte ... | Loaded ... |    3    |
        sf = sf.remove_column("reason")

        msg = _models_unavailable(sf)
        if msg:
            _logger.info(msg)
            return

        sf = sf.unpack('models', column_name_prefix = "")

        # Add expected and actual versions
        sf['expected version'] = sf.apply(lambda x: expected[x['name']]['version'] if expected.has_key(x['name']) else None )
        sf['actual'] = sf.apply(lambda x: '%s (%s)' % (x['version'], x['status']))

        # pivot the table by model and show one row for each model
        return sf \
            .groupby('name', {'state':_gl.aggregate.CONCAT('id', 'actual'), 'expected version':_gl.aggregate.SELECT_ONE('expected version')}) \
            .unpack('state', column_name_prefix='node')

    def _get_cache_status(self):
        '''
        Get the cache status for all deploy Predictive Objects on each node.

        Examples
        --------

        >>> ps._get_cache_status()
        +-----------------+-------------------------+-------------------------+
        |       name      |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+-------------------------+-------------------------+
        | Image Predictor |       1 (Enabled)       |      1 (Enabled)        |
        | Book Recommender|       0 (Disabled)      |      0 (Disabled)       |

        '''
        self._ensure_not_terminated()

        expected = self.deployed_predictive_objects
        result = self.get_status()
        if len(result) == 0:
            _logger.info('No nodes in the Predictive Service')
            return None

        # result is in the following format
        # [{'dns_name': u'ec2-54-191-36-143.us-west-2.compute.amazonaws.com',
        #   'id': u'i-29397b26',
        #   'models': [{u'description': u'None',
        #     u'name': u'Image Predictor',
        #     u'cache_enabled': True,
        #     u'status': u'Loaded successfully',
        #     u'version': 1}],
        #   'reason': u'N/A',
        #   'state': u'InService'},
        #   {...}
        #   ]
        sf = _gl.SArray(result)

        # unpack the dictionary to different columns, result looks like:
        # +------------+------------+--------+-----------+--------------------------------+
        # |  dns_name  |     id     | reason |   state   |             models             |
        # +------------+------------+--------+-----------+--------------------------------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        sf = sf.unpack(column_name_prefix="")

        # in case no model in any instance, there is no way to infer type of
        # 'models' column and the next stack operation would raise a TypeError
        msg = _models_unavailable(sf)
        if msg:
            _logger.info(msg)
            return

        sf = sf.stack('models', 'models')

        # expand the "models" column, results looks like:
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # |  dns_name  |     id     | reason |   state   | descri ... |    name    |   status   | version |
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService |    None    | Image  ... | Loaded ... |    1    |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | finds  ... | shorte ... | Loaded ... |    3    |
        sf = sf.remove_column('reason')

        msg = _models_unavailable(sf)
        if msg:
            _logger.info(msg)
            return

        sf = sf.unpack('models', column_name_prefix="")

        # Add the cache status (string)
        sf['cache_str'] = sf['cache_enabled'].apply(lambda x : "%s" % ("Enabled" if x == 1 else "Disabled"))
        sf['cache'] = sf.apply(lambda x : "%s (%s)" % (x['cache_enabled'], x['cache_str']))
        # pivot the table by model and show one row for each model
        return sf \
            .groupby('name', {'state':_gl.aggregate.CONCAT('id', 'cache')}) \
            .unpack('state', column_name_prefix='node')


    def get_status(self, view = None):
        '''
        Gets the status of the current Predictive Service.

        If parameter 'view' is given, will return an SFrame that shows specific
        aspect of the status.

        Parameters
        -----------
        view : 'cache'|'node'|'model'

        Returns
        --------
        out : list | SFrame
            If `view` is not given, then return a list with each element being
            a dictionary containing status information for one node. Available
            keys are: ['dns_name', 'id', 'models', 'state']

            If 'view' is 'cache', returns an SFrame showing cache status for each
            Predictive Object deployed in each node. Columns in SFrame include
            ['name', 'node.instance-id1', 'node.instance-id2', etc.], see sample
            output below. If no Predictive Object deployed yet, then returns None.

            If 'view' is 'node', returns an SFrame showing node status for each
            node in current Predictive Service. Columns in SFrame include
            ['dns_name', 'instance-id', 'state']

            If 'view' is 'model', returns an SFrame showing status for each
            deployed Predictive Object in each node. Columns in SFrame include
            ['name', 'expeted version', 'node.instance-id1', 'node.instance-id2',
            etc...], see sample output below. If no Predictive Object deployed yet,
            then returns None.

        Examples
        ---------

        This is a sample output for a Predictive Service with two nodes and two
        Predictive Objects deployed. With one cache enabled and one disabled.
        (Note: output omitted some SFrame header information.)

        >>> ps.get_status()
            [{'dns_name': 'ec2-54-148-131...',
              'id': 'i-2a397b25',
              'state': 'InService',
              'models': [
                {
                    u'cache_enabled': True,
                    u'description': u'',
                    u'name': u'Image Predictor',
                    u'status': u'Loaded successfully',
                    u'version': 1
                }, {
                    u'cache_enabled': False,
                    u'description': u'',
                    u'name': u'Book Recommender',
                    u'status': u'Loaded successfully',
                    u'version': 2
               }],
              },
             {'dns_name': 'ec2-54-186-64-...',
              'id': 'i-29397b26',
              'state': 'InService',
              'models': [
                {
                    u'cache_enabled': True,
                    u'description': u'',
                    u'name': u'Image Predictor',
                    u'status': u'Loaded successfully',
                    u'version': 1
                }, {
                    u'cache_enabled': False,
                    u'description': u'',
                    u'name': u'Book Recommender',
                    u'status': u'Loaded successfully',
                    u'version': 2
               }],
              },
            ]


        >>> ps.get_status(view = 'cache')
        +-----------------+-------------------------+-------------------------+
        |       name      |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+-------------------------+-------------------------+
        | Image Predictor |       1 (Enabled)       |      1 (Enabled)        |
        | Book Recommender|       0 (Disabled)      |      0 (Disabled)       |
        +-----------------+-------------------------+-------------------------+

        >>> ps.get_status(view = 'node')
        +--------------------+---------------+-----------+-----------+
        |      dns_name      |  instance_id  |   state   |   cache   |
        +--------------------+---------------+-----------+-----------+
        | ec2-54-148-131 ... |  i-2a397b25   | InService | Healthy   |
        | ec2-54-186-64- ... |  i-29397b26   | InService | Healthy   |
        +--------------------+---------------+-----------+-----------+


        >>> ps.get_status(view = 'model')
        +-----------------+------------------+-------------------------+-------------------------+
        |       name      | expected version |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+------------------+-------------------------+-------------------------+
        | Image Predictor |        1         | 1 (Loaded successfully) | 1 (Loaded successfully) |
        | Book Recommender|        2         | 2 (Loaded successfully) | 2 (Loaded successfully) |
        +-----------------+------------------+-------------------------+-------------------------+

        '''
        self._ensure_not_terminated()

        if not view:
            return self._environment.get_status()

        if view == 'cache':
            return self._get_cache_status()

        elif view == 'node':
            return self._get_nodes_status()

        elif view == 'model':
            return self.get_predictive_objects_status()
        else:
            raise ValueError("Supported views are: ['cache', 'node', 'model']," \
                "'%s' is not supported." % view)

    def set_query_timeout(self, timeout = 10):
        '''
        Set query timeout in seconds

        Parameters
        -----------
        timeout : int
            The timeout (in seconds) of the query to the Predictive Service.

        Examples
        ---------

            >>> deployment.set_query_timeout(30)

        '''
        self._ensure_not_terminated()

        if timeout <= 0 or not isinstance(timeout, int):
            raise ValueError('"timeout" value has to be a positive integer in seconds.')

        self.query_timeout = timeout

    def query(self, po_name, **kwargs):
        '''
        Submits a query to the deployed PredictiveObject with the name specified
        by `po_name`.

        Parameters
        ----------
        po_name : str
            The name of the Predictive Object to query

        kwargs : kwargs
            The keyword arguments passed into Predictive Object query method.
            For Custom Predictive Object, the kwargs depends on custom query
            signature, for built-in Model Predictive Object, the kwargs has to
            be in the following format:

                >>> deployment.query(po_name, method=<method>, data = <data-dictionary>)

        Returns
        -------
        out : dict
            For successful requests, returns a dictionary containing the
            following fields:

            - "uuid": a unique request identifer string

            - "response": the actual result of the query returned by the model

            - "version": an integer corresponding to the version number of the
            model that generated the result

        See Also
        ---------
        test_query

        Examples
        ---------
        If there is a Predictive Object named 'book recommender' and we want to
        recommend some books for users, the following query may be used:

          >>> deployment.query('book recommender',
          ...                   method = 'recommend',
          ...                   data = {
          ...                       'users': [
          ...                           {'user_id':12345, 'book_id':2},
          ...                           {'user_id':12346, 'book_id':3},
          ...                       ],
          ...                       'k': 5
          ...                   }
          ...                 )


        '''
        self._ensure_not_terminated()

        try:
            timeout = self.query_timeout if hasattr(self, 'query_timeout') else 10
            return self._environment.query(po_name, self.api_key, timeout=timeout, **kwargs)
        except _NonExistError:
            return "Predictive Object '%s' can not be found. If you just deployed "\
                "the Predictive Object, it may take a short while for all Predictive " \
                "Service nodes to be up-to-date. Please use get_predictive_objects_status() " \
                "to get most current state." % po_name

    def test_query(self, po_name, **kwargs):
        '''
        Used for simulating querying the given Predictive Object locally without
        going to actual Predictive Service nodes.

        This method queries the specified Predictive Object with the input and
        validates the end to end query to this Predictive Object locally.

        Unlike the `query` method, which submits a query to a deployed
        Predictive Service cluster, this method runs entirely localy.

        The purpose is to validate the Predictive Object logic before actually
        deploying the Predictive Object to production.

        Parameters
        ----------
        po_name : str
            The name of the Predictive Object to query

        kwargs : Any
            The keyword arguments passed into Predictive Object query method

        Returns
        -------
        out : dict
            For successful requests, returns a dictionary containing the
            following fields:

            - "uuid": a unique request identifer string

            - "response": the actual result of the query returned by the model

            - "version": an integer corresponding to the version number of the
            model that generated the result

        See Also
        ---------
        query

        Examples
        ---------
        If there is a deployed Predictive Object named 'book recommender' and we want to
        validates our input query with this Predictive object, the following query may
        be used:

          >>> deployment.test_query('book recommender',
          ...                        method = 'recommend',
          ...                        data = {
          ...                          'users': [
          ...                                {'user_id':12345, 'book_id':2},
          ...                                {'user_id':12346, 'book_id':3},
          ...                          ],
          ...                          'k': 5
          ...                        }
          ...                      )

        '''
        self._ensure_not_terminated()

        uuid = unicode(_random_uuid())

        # simulate server side logging
        def custom_logger(**kwargs):
            _logger.info(_json.dumps({'uri':po_name, 'version':1, 'uuid': uuid, 'data':kwargs}))

        # serialize then deserialize data
        try:
            validated_data = _json.loads(_json.dumps(kwargs))
        except:
            raise ValueError("\"data\" parameter cannot be serialized into JSON.")
        _logger.info("Input data serializable.")

        unpickled_po = None

        # load Predictive Object from local changes
        temp_dir = None
        try:
            if self._local_changes.has_key(po_name):
                po = self._local_changes[po_name][0]
                temp_dir = _gl.util._make_temp_filename(prefix='predictive_object_')
                po.save(temp_dir) # pickle and save
                unpickled_po = _PredictiveObject.load(temp_dir, po.schema_version) # load and unpickle
            elif self._predictive_objects.has_key(po_name):
                # load deployed Predictive Object from Predictive Service
                unpickled_po = self.load_predictive_object(po_name)
            else:
                raise ValueError('Cannot find Predictive Object with name "%s".' % po_name)

            # test the query
            if hasattr(unpickled_po, 'custom_query'):
                unpickled_po.custom_query.log = custom_logger
            result = unpickled_po.query(**validated_data)

            # serialize the result
            try:
                serialized_result = _json.dumps(result)
            except:
                raise ValueError("Result cannot be serialized to JSON format, please check "
                                "your Model or your Custom Predictive Object to make sure "
                                " the result is in JSON format.")
            _logger.info("Query results serializable.")

            return {u'uuid':uuid, u'response':result, u'version':1}
        finally:
            if temp_dir is not None:
                _shutil.rmtree(temp_dir)

    def feedback(self, request_id, **kwargs):
        '''
        Submits feedback data corresponding to a particular query result.

        The feedback method is intended to allow PredictiveService clients to
        associate arbitrary data with the results returned from a particular
        query. For example, you could store click data (user ID, clicked result
        rank, etc.) in order to evaluate your model and train the next version.

        Parameters
        ----------
        request_id : str
            The unique request identifer associated with a particular query and
            result (ie. the ```uuid``` field).

        kwargs : kwargs
            Named attribute and value pairs to associate with a query and result.
            The benefit of keyword arguments is their flexibility; users can
            record any arbitrary key-value pairs with a particular query-result
            pair for future analysis.

        See Also
        ---------
        get_feedback_logs, get_query_logs, get_result_logs

        Examples
        ---------
        The following examples assume the existence of an autocomplete model,
        where the model is queried to produce suggested completions for a user's
        text input. The user then either accepts or rejects the suggestion,
        which serves as the basis for a feedback call.

          >>> deployment.feedback('e8f13b17-173a-402d-835d-cc816eba626f',
                                  search_term='anomoly',
                                  suggested='anomoly detection',
                                  suggestion_accepted=True)

          >>> deployment.feedback('b7a620c8-14a9-4d66-a6a0-9c3308dffe1b',
                                  search_term='accomodat',
                                  suggested='accommodations',
                                  suggestion_accepted=False,
                                  next_search_term='accommodate')

        When it comes time to train a new model, you could use rows in the
        feedback logs as training data. For illustration purposes, the example
        above contains more data than is strictly required. You could exclude
        some of the fields above (eg. 'search_term', 'suggested') from the
        feedback, since it could just as easily be collected from the query and
        result logs, which can be joined with the feedback log on the ```uuid```
        field.

        '''
        self._ensure_not_terminated()

        try:
            timeout = self.query_timeout if hasattr(self, 'query_timeout') else 10
            result = self._environment.feedback(request_id, self.api_key,
                                                timeout=timeout, **kwargs)
            result.raise_for_status()
        except Exception as e:
            _logger.error("Error submitting feedback: %s" % e.message)

    def add(self, name, obj, description=''):
        '''
        Adds a new model or custom predictive object to this predictive service.

        The name of the predictive object must be unique to the Predictive
        Service. This operation will not take effect until `apply_changes` is
        called.

        Parameters
        ----------
        name : str
            A unique identifier for the predictive object. This name is used
            when querying the Predictive Service deployment.

        obj : Model | function | path (local or S3) to a saved Model
            The Predictive Object to add to this Predictive Service.

            The Predictive Object can either be a GraphLab Create Model or a
            custom user-defined function.

            If 'obj' is a Model, then it can either be an actual GraphLab Create
            Model object, or it can be a path to a place where the model is saved.

            If 'obj' is a custom defined query, the query can have any signature.
            But input and output of the query needs to be JSON serializable

            If the query needs to use any python packages, then use required_packages
            decorator. Here is an example of add() query:

                >>> from graphlab.deploy import required_packages
                >>> @required_packages(["dep1==1.2.2", "dep2=2.3.4"])
                >>> def add(a, b):
                >>>     return a + b

        description : str
            The description of the custom predictive object, optional.

        See Also
        --------
        apply_changes

        Notes
        -----
        This operation will not take effect until `apply_changes` is called.

        Examples
        --------
        To add a GraphLab Create Model:

            >>> model = graphlab.recommender.create(...)
            >>> ps.add('recommender', model)

        To test query the newly added recommender

            >>> ps.apply_changes()
            >>> ps.query('recommender', method='recommend', data={'users':'abc'})

        To add a GraphLab Create Model saved locally / S3:

            >>> ps.add('recommender', '~/saved_models/recommender')
            >>> ps.add('recommender2', 's3://model-archive/recommender')

        To add a custom Predictive Object:

            >>> def add(a, b):
            >>>     return a + b
            >>>
            >>> ps.add('add_two_numbers', add)

        After deploy to production, the query can be queried the following way:

            >>> ps.apply_changes()
            >>> ps.query('add_two_numbers', a = 1, b = 2)

        To add a custom Predictive Object that has GraphLab Create model dependency:

            >>> recommender = graphlab.recommender.create(...)
            >>> def my_recommender(user_id):
            >>>
            >>>     if not isinstance(user_id, str):
            >>>         raise ValueError('"user_id" has to be a string')
            >>>     return recommender.recommend([user_id])
            >>>
            >>> ps.add('custom-recommender', my_recommender)

            >>> ps.apply_changes()
            >>> ps.query('custom-recommender', user_id = 'abc')

        If the custom query depends on other Python package(s), the dependendent
            package(s) may be declared using @required_packages decorator:

            >>> from graphlab.deploy import required_packages
            >>> @required_packages(["names", ...])
            >>> def generate_names(num_names):
            >>>     import names
            >>>     return gl.SArray([names.get_full_name() for i in range(num_items)])

        '''
        self._ensure_not_terminated()

        if not isinstance(name, str):
            raise TypeError("'name' must be a string")

        if name == None or name == '':
            raise TypeError("'name' cannot be empty")

        if name in self.deployed_predictive_objects.keys():
            raise ValueError("There is already a predictive object with name '%s'." % name)

        # check for invalid PO names
        _check_PO_name(name)

        predictive_object = None
        if isinstance(obj, _types.FunctionType):
            predictive_object = self._create_custom_po(obj, description)
        elif isinstance(obj, _gl.Model) or isinstance(obj, _gl.CustomModel) or \
          _is_path(obj):
            predictive_object = ModelPredictiveObject(model=obj)
        elif isinstance(obj, _PredictiveObject):
            predictive_object = obj
        else:
            raise TypeError("'obj' parameter has to be either an instance of " \
                            "GraphLab Create model, a path to a saved GraphLab " \
                            "Create model, or an instance of GraphLab Create PredictiveObject")

        # extract doc string
        if not _is_path(obj):
            docstring = predictive_object.get_doc_string().strip()
        else:
            docstring = ''
        self._local_changes[name] = (
            predictive_object,
            {   'version':1,
                'docstring':docstring,
                'description':predictive_object.description,
                'cache_state': self._global_cache_state, # defaults to global cache state for new PO
                'schema_version':predictive_object.schema_version
            }
        )

        _logger.info("New predictive object '%s' added, use apply_changes() to deploy all pending changes, or continue other modification." % name)

    def update(self, name, obj, description = ''):
        '''
        Updates the version of the predictive object being served.

        Parameters
        ----------
        name : str
            The unique identifier for the model.

        obj : Model | function | path (local or S3) to a saved Model
            The Predictive Object to add to this Predictive Service.

            The Predictive Object can either be a GraphLab Create Model or a
            custom function.

            If 'obj' is a custom defined query, the query can have any signature.
            But input and output of the query needs to be JSON serializable

        See Also
        --------
        add, apply_changes

        Notes
        -----
        This operation will not take effect until `apply_changes` is called.

        Examples
        --------
        To update a predictive object named 'recommender':

            >>> new_recommender = graphlab.recommender.create(...)
            >>> ps.update('recommender', new_recommender)

        '''
        self._ensure_not_terminated()

        new_version = 1
        cache_state = self._global_cache_state
        if name not in self.deployed_predictive_objects.keys():
            if name not in self.pending_changes.keys():
                raise ValueError("Cannot find a predictive object with name '%s'." % name)
            else:
                new_version = 1
        else:
            new_version = 1 + self._predictive_objects[name]['version']
            cache_state = self._predictive_objects[name]['cache_state']

        predictive_object = None
        if isinstance(obj, _types.FunctionType):
            predictive_object = self._create_custom_po(obj, description)
        elif isinstance(obj, _gl.Model) or isinstance(obj, _gl.CustomModel) or \
          _is_path(obj):
            predictive_object = ModelPredictiveObject(model=obj)
        elif isinstance(obj, _PredictiveObject):
            predictive_object = obj
        else:
            raise TypeError("'obj' parameter has to be either an instance of GraphLab Create model, a path to a saved GraphLab Create model, or an instance of GraphLab Create PredictiveObject")

        if not _is_path(obj):
            docstring = predictive_object.get_doc_string().strip()
        else:
            docstring = ''
        self._local_changes[name] = (
            predictive_object,
            {   'version':new_version,
                'docstring':docstring,
                'description':predictive_object.description,
                'cache_state':cache_state,
                'schema_version':predictive_object.schema_version
            }
        )

        _logger.info("Predictive object '%s' updated to version '%s', use apply_changes() to deploy all pending changes, or continue other modification." % (name, new_version))

    def remove(self, name):
        '''
        Removes a deployed Predictive Object

        Parameters
        ----------
        name : str
            The name of the Predictive Object to be removed

        Notes
        -----
        This operation will not take effect until `apply_changes` is called.

        See Also
        --------
        apply_changes
        '''
        self._ensure_not_terminated()

        if name not in self.deployed_predictive_objects.keys():
            if name in self.pending_changes.keys():
                del self._local_changes[name]
            else:
                raise ValueError("Cannot find a model with name '%s'." % name)
        else:
            self._local_changes[name] = (None, None)

        _logger.info("Predictive object '%s' removed, use apply_changes() to deploy all pending changes, or continue other modification." % name)

    def apply_changes(self):
        '''
        Applies all pending changes to this Predictive Service deployment.

        If apply_changes returns success, then all pending changes have been
        staged to the predictive objects path associated with the Predictive
        Service. Each of the nodes in the Predictive Service will pick up
        the state eventually.

        Use get_status() to check the status of each node.

        See Also
        --------
        add, update, remove
        '''
        self._ensure_not_terminated()

        if len(self._local_changes) == 0:
            _logger.info("There are no pending changes.")
            return

        self._save_predictive_objects()
        self._save_state()
        try:
            self._environment.poke()
        except _ConnectionError as e:
            _logger.warn("Unable to connect to running Predictive Service: %s" %
                         (e.message))
        self._update_local_state()


    def clear_pending_changes(self):
        '''
        Clears all changes which have not been applied.

        Clears all actions done using `add`, `update` and `remove`, since the
        last time `apply_changes` was called.
        '''
        self._ensure_not_terminated()

        _logger.info('Clearing all pending changes.')
        self._local_changes = {}


    def save_client_config(self, file_path, predictive_service_cname):
        '''
        Create the config file that can be used by applications accessing the
        Predictive Service.

        The file is stored in an .ini format, with the specific information
        necessary to query this deployment.

        Parameters
        ----------
        file_path : str
            The path where the config file will be saved.

        predictive_service_cname : str
            The CNAME for the Predictive Service endpoint. It is *highly recommended*
            that all client connect through a CNAME record that you have created.
            If this value is set to None, the "A" record will be used.
            Using the "A" record is only advisable for testing, since this value
            may change over time.
        '''
        self._ensure_not_terminated()

        out = _ConfigParser(allow_no_value=True)
        out.optionxform = str
        out.add_section(PredictiveService._SERVICE_INFO_SECTION_NAME)
        out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'api key', self.api_key)

        if hasattr(self._environment, 'certificate_name') and self._environment.certificate_name:
            out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'verify certificate',
                    not(self._environment.certificate_is_self_signed))
            schema = 'https://'
        else:
            schema = 'http://'

        if predictive_service_cname:
            out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'endpoint',
                    schema + predictive_service_cname)
        else:
            _logger.warn('Creating client config using the "A" Record name. These types of records'
                         ' often change over time. It is strongly recommended that you create a'
                         ' CNAME record for this load balancer and use that. It would be crazy'
                         ' not to use CNAME records for all production purposes.')
            out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'endpoint',
                    schema + self._environment.load_balancer_dns_name)

        with open(file_path, 'w') as f:
            out.write(f)

    def _recover_cache(self, max_retries=3):
        '''
        Restart the cache and restore the cache state to the cache settings
        in place before the restart.

        Parameters
        ----------
        max_retries : int
            The number of times we should re-attempt to enable the cache if it
            doesn't start cleanly.
        '''
        def toggle(is_enabled, po_name):
            if is_enabled:
                self._environment.cache_enable(po_name)
            else:
                self._environment.cache_disable(po_name)

        # capture cache state before disabling/enabling
        cache_state = {po_name: _is_cache_enabled(status) for
                       (po_name, status) in self._predictive_objects.items()}

        # disable the cache
        self._environment.cache_disable(None)
        self._environment.cache_enable(None, True)

        # if failed, retry as many as num_retries attempts
        num_retries = 0
        healthy = "healthy"

        while not self._environment._is_cache_ok(healthy) and \
          num_retries < max_retries:
            _logger.warn("Cluster cache unstable; restarting")
            self._environment.cache_enable(None, True)
            num_retries += 1
            _time.sleep(2)

        # reset the cache state
        if self._environment._is_cache_ok(healthy):
            for po_name, is_enabled in cache_state.items():
                toggle(is_enabled, po_name)
        else:
            _logger.error("Predictive service cluster is unstable: cache is " \
                          "unhealthy")

    def _remove_nodes(self, instance_ids, cache_restart=True):
        '''
        Terminate one or more nodes in a Predictive Service cluster by their
        instance IDs, and remove them from the cluster.

        Parameters
        ----------
        cache_restart : bool
            A boolean indicating whether we should restart the cache. This
            defaults to True. Restarting the cache results in the clearing of
            any existing cached results. However, after adding or removing
            nodes from the cluster, the cache will not function without a
            restart.
        '''
        if not isinstance(instance_ids, list):
            instance_ids = [instance_ids]

        if not all([type(i) in [str, unicode] for i in instance_ids]):
            raise TypeError('The "instance_ids" parameter must be either a ' \
                            'string or a list of strings')

        instance_ids = list(set(instance_ids)) # ensure unique elements

        node_ids = {x.id for x in self._environment._get_all_hosts()}
        num_nodes_deployed = len(node_ids)

        # ensure we're not removing any nodes outside this predictive service
        if not all([instance_id in node_ids for instance_id in instance_ids]):
            raise ValueError("Cannot remove a node that is not in this " \
                             "PredictiveService cluster.")

        if num_nodes_deployed == len(instance_ids):
            cache_restart = False # there will be no instances left
            _logger.warn("This action will terminate all of the nodes in " \
                         "your predictive service cluster.")

        self._environment.terminate_instances(instance_ids)

        if self._global_cache_state == "enabled" and cache_restart:
            self._recover_cache()

    def remove_nodes(self, instance_ids):
        '''
        Terminate one or more nodes in a Predictive Service cluster by their
        instance IDs, and remove them from the cluster.

        This is useful in the event of a node failure. You can query the
        Predictive Service status with `get_status`. If there are unresponsive
        nodes, capture their instance IDs and pass them as the sole parameter
        to this function.

        Parameters
        ----------
        instance_ids : list[str] | str
            The instance IDs for the nodes to terminate.

        See Also
        --------
        get_status, add_nodes, cache_enable

        Notes
        -----
        Calling this method will clear out any existing data in the distributed
        cache.

        Examples
        --------
        To remove a node in the current Predictive Service:

          >>> print ps.get_status('node')
          +---------+-------------------------------+-------------+-----------+
          |  cache  |            dns_name           | instance_id |   state   |
          +---------+-------------------------------+-------------+-----------+
          | Healthy | ec2-12-11-12-121.us-west-2... |  i-4399eb8a | InService |
          | Healthy | ec2-21-21-24-221.us-west-2... |  i-ccc4663a | InService |
          +---------+-------------------------------+-------------+-----------+
          [2 rows x 4 columns]
          >>> ps.remove_nodes(['i-ccc4663a'])
          >>> print ps.get_status('node')
          +---------+-------------------------------+-------------+-----------+
          |  cache  |            dns_name           | instance_id |   state   |
          +---------+-------------------------------+-------------+-----------+
          | Healthy | ec2-12-11-12-121.us-west-2... |  i-4399eb8a | InService |
          +---------+-------------------------------+-------------+-----------+
          [1 rows x 4 columns]

        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()
        self._remove_nodes(instance_ids)

    def _add_nodes(self, num_nodes, instance_type=None,
                   security_group_name=None, CIDR_rule=None, tags=None,
                   cache_restart=True):
        '''
        Add one or more nodes to a Predictive Service Cluster.

        Parameters
        ----------
        num_nodes : int
            The number of additional nodes to add to the cluster. The maximum
            number of nodes that can be launched at one time is 5. If the
            `num_nodes` parameter exceeds that limit, a ValueError is thrown.

        instance_type : str, optional
            The type of instance to launch. If specified, this must match the
            instance type used when the service was originally launched.

        security_group : str, optional
            The name of the security group for the EC2 instance to use. If
            specified, this must match the instance type used when the service
            was originally launched.

        CIDR_rule : string or list[string], optional
            The Classless Inter-Domain Routing rule(s) to use for the instance.
            Useful for restricting the IP Address Range for a client. Default is
            no restriction. If you specify CIDR_rule(s), you must also specify a
            security group to use.

        tags : dict, optional
            A dictionary containing the name/value tag pairs to be assigned to
            the instance. If you want to create only a tag name, the value for
            that tag should be the empty string (i.e. ''). In addition to these
            specified tags, a 'GraphLab' tag will also be assigned.

        cache_restart : bool
            A boolean indicating whether we should restart the cache. This
            defaults to True. Restarting the cache results in the clearing of
            any existing cached results. However, after adding or removing
            nodes from the cluster, the cache will not function without a
            restart.
        '''

        if num_nodes <= 0:
            _logger.warn("The num_nodes parameter must have a positive " \
                         "value for this method to have any effect.")
            return

        if num_nodes > NODE_LAUNCH_LIMIT:
            raise ValueError("You cannot launch more than %d nodes. If this " \
                             "limit is problematic, please contact " \
                             "support@dato.com" % NODE_LAUNCH_LIMIT)

        if not instance_type:
            attrs = self._environment._get_instance_attributes()
            instance_type = attrs['instance_type']
            security_group_name = attrs['security_group_name']
            tags = attrs['tags']

        self._environment.add_instances(self._state_path, num_nodes,
                                        instance_type, security_group_name,
                                        tags, CIDR_rule=CIDR_rule,
                                        additional_port_to_open=self.port)

        if self._global_cache_state == "enabled" and cache_restart:
            self._recover_cache()

    def add_nodes(self, num_nodes):
        '''
        Add one or more nodes to a Predictive Service Cluster.

        Parameters
        ----------
        num_nodes : int
            The number of additional nodes to add to the cluster. The maximum
            number of nodes that can be launched at one time is 5. If the
            `num_nodes` parameter exceeds that limit, a ValueError is thrown.

        Notes
        -----
        Calling this method will clear out any existing data in the distributed
        cache.

        Examples
        --------
        To add three new nodes to the current Predictive Service:

          >>> ps.add_nodes(3)

        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()
        self._add_nodes(num_nodes)

    def replace_nodes(self, instance_ids):
        '''
        Replace each of the nodes specified in `instance_ids` with a new node.

        For each node terminated, add a new one in its place. After launching
        new nodes, terminate those specified by the `instance_ids` parameter.

        Parameters
        ----------
        instance_ids : list[str] | str
            The instance IDs for the nodes to terminate.

        See Also
        --------
        add_nodes, remove_nodes

        Notes
        -----
        Calling this method will clear out any existing data in the distributed
        cache.

        Examples
        --------
        To replace a node in the current Predictive Service with a new node:

          >>> print ps.get_status('node')
          +---------+-------------------------------+-------------+-----------+
          |  cache  |            dns_name           | instance_id |   state   |
          +---------+-------------------------------+-------------+-----------+
          | Healthy | ec2-12-11-12-121.us-west-2... |  i-4399eb8a | InService |
          | Healthy | ec2-21-21-24-221.us-west-2... |  i-ccc4663a | InService |
          +---------+-------------------------------+-------------+-----------+
          [2 rows x 4 columns]
          >>> ps.replace_nodes(['i-ccc4663a'])

        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()

        if not isinstance(instance_ids, list):
            instance_ids = [instance_ids]

        if not all([type(i) in [str, unicode] for i in instance_ids]):
            raise TypeError('The "instance_ids" parameter must be either a ' \
                            'string or a list of strings')

        num_nodes_deployed = len(self._environment._get_all_hosts())
        num_replacements = len(instance_ids)

        if num_replacements == num_nodes_deployed:
            _logger.warn("Replacing all %d existing nodes in cluster" \
                         % num_replacements)

        # bring up replacement nodes
        _logger.info("Adding %d nodes to cluster", num_replacements)

        num_replaced = 0
        while num_replaced < num_replacements:
            n = min(num_replacements - num_replaced, NODE_LAUNCH_LIMIT)
            self._add_nodes(n, cache_restart=False)
            num_replaced += n

        # terminate specified nodes
        self._remove_nodes(instance_ids, cache_restart=False)

        # try re-enabling cache
        if self._global_cache_state == "enabled":
            self._recover_cache()

        # check that cache is healthy, and warn if not
        expected_cache_status = "healthy" if self._global_cache_state else "disabled"
        if self._environment._is_cache_ok(expected_cache_status):
            _logger.info("Predictive service cluster is healthy")

    def terminate_service(self, remove_logs=False, remove_state=True):
        '''
        Terminates the Predictive Service deployment.

        This will terminate all EC2 hosts and delete the load balancer. The
        Predictive Service object is not usable after terminate_service is
        called.

        **This operation can not be undone.**

        Parameters
        ----------
        remove_logs : bool
            Delete all logs associated with this Predictive Service Deployment remotely.

        remove_state : bool
            Delete all state data associated with this Predictive Service Deployment remotely.
        '''

        self._raise_error_if_on_premise_ps();
        self._ensure_not_terminated()

        # Terminate hosts and delete load balancer.
        self._environment.terminate(remove_logs)

        # Remove state file, stored predictive objects and dependencies
        if remove_state:
            self._environment.remove_state(self._state_path,
                                            PredictiveService._DEPENDENCIES_DIR,
                                            PredictiveService._PREDICTIVE_OBJECT_DIR)

        # Remove environment object
        self._environment = None

        # Delete local predictive service endpoint
        _gl.deploy.predictive_services.delete(self.name)

    def _get_latest_state(self):
        '''
        Update the predictive objects from the latest remote state file.

        Parameters
        ----------
        config : config, optional
            The configuration file given manually to update to the latest state.

        Returns
        -------
        out : dict
            Returns a summary of what has changed. There is one entry in the dictionary for each
            predictive object that has changed. The keys are the name of the predictive object. The
            values are tuples: (model_version_number, path_to_new_version).
        '''
        # TODO: make sure we're not "ahead" of remote state file due to eventual consistency

        self._ensure_not_terminated()

        remote_cfg = PredictiveServiceEnvironment._get_state_from_file(self._state_path, self.aws_credentials)

        if remote_cfg.has_option(PredictiveService._META_SECTION_NAME, 'Schema Version'):
            self._schema_version = remote_cfg.getint(PredictiveService._META_SECTION_NAME, 'Schema Version')
        else:
            # if missing schema version, fail
            raise ValueError('Invalid remote state configuration file; missing ' \
                             '"Schema Version"')

        # if remote schema version is 1, fail
        if self._schema_version == 1:
            raise ValueError("The Predictive Service that you are trying to " \
                             "load is running version 1, which is no " \
                             "longer supported. Please re-create your " \
                             "Predictive Service using your current version " \
                             "of GraphLab Create.")

        # if remote schema version is > than ours, fail
        if self._schema_version > PREDICTIVE_SERVICE_SCHEMA_VERSION:
            raise ValueError("Your GraphLab Create only supports Predictive " \
                             "Services with schema version up to '%s', " \
                             "while the Predictive Service you are trying " \
                             "to load has schema version '%s'. Please " \
                             "upgrade GraphLab Create to the latest " \
                             "version." % (PREDICTIVE_SERVICE_SCHEMA_VERSION,
                                           self._schema_version))

        # warn if there's a version mismatch
        if self._schema_version != PREDICTIVE_SERVICE_SCHEMA_VERSION:
            _logger.warn("There is a version mismatch between what is " \
                         "expected by this version of GraphLab Create " \
                         "(%d) and your deployed Predictive Service (%d)" \
                         ". Some client functionality may not be supported " \
                         "by the server." % (
                             PREDICTIVE_SERVICE_SCHEMA_VERSION,
                                   self._schema_version))

        # update global cache metadata and cors_origin metadata
        if remote_cfg.has_option(PredictiveService._SERVICE_INFO_SECTION_NAME, 'CORS Origin'):
            self._cors_origin = remote_cfg.get(PredictiveService._SERVICE_INFO_SECTION_NAME, 'CORS Origin')
        if remote_cfg.has_option(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Global Cache State'):
            self._global_cache_state = remote_cfg.get(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Global Cache State')

        updated_deployment_versions = {}
        model_names = remote_cfg.options(PredictiveService._DEPLOYMENT_SECTION_NAME)
        for cur_model_name in model_names:
            model_info = _json.loads(remote_cfg.get(PredictiveService._DEPLOYMENT_SECTION_NAME,
                                                    cur_model_name))
            updated_deployment_versions[cur_model_name] = model_info

        if remote_cfg.has_section(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING):
            for cur_model_name in model_names:
                cur_model_docstring = remote_cfg.get(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING,
                                                     cur_model_name)
                updated_deployment_versions[cur_model_name]['docstring'] = cur_model_docstring.decode('string_escape')

        if remote_cfg.has_section(PredictiveService._ENVIRONMENT_SECTION_NAME):
            # Create and attach the environment.
            environment_info = dict(remote_cfg.items(PredictiveService._ENVIRONMENT_SECTION_NAME))
            if hasattr(self._environment, 'aws_credentials'):
                environment_info['aws_credentials'] = self._environment.aws_credentials
            self._environment = predictive_service_environment_factory(environment_info)
        else:
            self._environment = None

        diff = {}
        for (cur_po_name, cur_po_info) in updated_deployment_versions.items():
            if(cur_po_name not in self._predictive_objects or
               cur_po_info['version'] != self._predictive_objects[cur_po_name]['version']):
                # Either a new predictive object or new predictive object version
                path_to_new_version = self._get_predictive_object_save_path(cur_po_name,
                                                                            cur_po_info['version'])
                diff[cur_po_name] = (cur_po_info['version'], path_to_new_version)

        # add removed models too
        for (po_name, po_info) in self._predictive_objects.iteritems():
            if po_name not in updated_deployment_versions.keys():
                diff[po_name] = (None, None)

        self._revision_number = remote_cfg.getint(PredictiveService._META_SECTION_NAME, 'Revision Number')

        self._predictive_objects = updated_deployment_versions
        self._local_changes = {}

        return diff

    def get_metrics_url(self):
        '''
        Return a URL to Amazon CloudWatch for viewing the metrics assoicated with
        the service

        Returns
        -------
        out : str
            A URL for viewing Amazon CloudWatch metrics
        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()

        _logger.info("retrieving metrics from predictive service...")
        try:
            return self._environment.get_metrics_url(self.name)
        except Exception as e:
            _logger.error("Error retrieving metrics: %" % e.message)

    def get_metrics(self, po_name=None, start_time=None, end_time=None, time_range=None, period=None):
        '''
        Get the metrics associated with the Predictive Service instance. The metrics include
        number of requests, latency, number of healthy hosts, and number of keys in cache.

        Parameters
        ----------
        po_name : str, optional
            The name of the predictive object, used to get the metrics specific to this
            predictive object. If None, the metrics for the entire Predictive Service
            would be returned.

        start_time : datetime or str, optional
            The begin time to query metrics.

        end_time : datetime or str, optional
            The end time to query metrics.

        time_range : int , optional
            The range of time in hours to query metrics. Default is 12 hours.

        period : int, optional
            The period of data points in seconds for query metrics. Default is 5 minutes.

        Returns
        -------
        out : tuple (SFrames)
            Returns a tuple of SFrames containing the SFrames for each metric:
            number of requests, latency, number of healthy hosts, number of keys in cache.
            It also contains a SFrame with the start and end time for this metrics query.

        Notes
        -----
        If none of the parameters are given, this function would return the metrics for
        the last 12 hours.

        Examples
        --------
        To get the last 12 hours of metrics, with 5 minute (300 seconds) period:

        >>> ps.get_metrics()
        ({'end_time': datetime.datetime(2014, 11, 13, 11, 36, 3, 157682),
          'start_time': datetime.datetime(2014, 11, 12, 23, 36, 3, 157682)},
        +------------------+---------------------------+-------------------+
        | num_requests.Sum |   num_requests.Timestamp  | num_requests.Unit |
        +------------------+---------------------------+-------------------+
        |       8.0        | 2014-11-13 00:31:00+00:00 |       Count       |
        |       2.0        | 2014-11-13 00:36:00+00:00 |       Count       |
        |       7.0        | 2014-11-13 00:41:00+00:00 |       Count       |
        |     24707.0      | 2014-11-13 00:46:00+00:00 |       Count       |
        |       5.0        | 2014-11-13 00:51:00+00:00 |       Count       |
        ......

        To get the last 6 hours of metrics, with 1 minute (60 seconds) period:

        >>> ps.get_metrics(end_time = datetime.datetime.now(), time_range = 6, period = 60)
        ({'end_time': datetime.datetime(2014, 11, 13, 11, 49, 7, 743095),
          'start_time': datetime.datetime(2014, 11, 13, 5, 49, 7, 743095)},
        +------------------+---------------------------+-------------------+
        | num_requests.Sum |   num_requests.Timestamp  | num_requests.Unit |
        +------------------+---------------------------+-------------------+
        |       1.0        | 2014-11-13 05:52:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 05:54:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 05:55:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 06:04:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 06:06:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 06:19:00+00:00 |       Count       |
        ......

        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()

        # check if po_name is valid
        if po_name and po_name != "cache" and not self._predictive_objects.has_key(po_name):
            raise ValueError('Cannot find Predictive Object with name "%s".' % po_name)

        # default time range of metrics to 12 hours
        if not time_range or type(time_range) != int:
            time_range = 12

        # convert timestamps to datetimes if necessary
        if start_time is not None:
            if type(start_time) == str or type(start_time) == unicode:
                start_time = _datetime.datetime.strptime(start_time.split(".")[0], '%Y-%m-%dT%H:%M:%S')
        if end_time is not None:
            if type(end_time) == str or type(end_time) == unicode:
                end_time = _datetime.datetime.strptime(end_time.split(".")[0], '%Y-%m-%dT%H:%M:%S')

        # get actual start_time and end_time
        if start_time is not None and end_time is not None:
            pass
        elif start_time is not None and end_time is None:
            end_time = start_time + _datetime.timedelta(hours=time_range)
        elif end_time is not None and start_time is None:
            start_time = end_time - _datetime.timedelta(hours=time_range)
        else:
            end_time = _datetime.datetime.utcnow()
            start_time = end_time - _datetime.timedelta(hours=time_range)

        # default period to 5 min
        if not period or type(period) != int:
            period = 300

        # get CloudWatch metrics
        metrics = self._environment._get_metrics(self.name, po_name, start_time, end_time, period)
        if not metrics:
            return None

        result = {}
        for key in metrics:
            m = map(dict, sorted(metrics[key]))
            if len(m) > 0:
                sort_column_name = key + ".Timestamp"
                result[key] = _gl.SArray(m).unpack(key).sort(sort_column_name)

        return result

    def get_query_logs(self, start_time, end_time):
        """
        Fetch query logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_result_logs, get_feedback_logs, get_server_logs, get_custom_logs,
        get_graphlab_service_logs, graphlab.deploy.job.create

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all query logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get query logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_query_logs(start_time=start_time, end_time=None)

        Get query logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_query_logs(start_time=start_time, end_time=end_time)

        Get query logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_query_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("query", start_time, end_time)

    def get_result_logs(self, start_time, end_time):
        """
        Fetch result logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.


        See Also
        --------
        get_query_logs, get_feedback_logs, get_server_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all result logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get result logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_result_logs(start_time=start_time, end_time=None)

        Get result logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_result_logs(start_time=start_time, end_time=end_time)

        Get result logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_result_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("result", start_time, end_time)

    def get_feedback_logs(self, start_time, end_time):
        """
        Fetch feedback logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_server_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all feedback logs, but you probably don't want to. If you have
        an operation that you wish to run over the entire set of logs, you
        should launch it as a remote `Job`.

        Examples
        --------
        Get feedback logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_feedback_logs(start_time=start_time, end_time=None)

        Get feedback logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_feedback_logs(start_time=start_time, end_time=end_time)

        Get feedback logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_feedback_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("feedback", start_time, end_time)

    def get_server_logs(self, start_time, end_time):
        """
        Fetch server logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_feedback_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all server logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get server logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_server_logs(start_time=start_time, end_time=None)

        Get server logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_server_logs(start_time=start_time, end_time=end_time)

        Get server logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_server_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("server", start_time, end_time)

    def get_custom_logs(self, start_time, end_time):
        """
        Fetch custom logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_feedback_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all custom logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get custom logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_custom_logs(start_time=start_time, end_time=None)

        Get custom logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_custom_logs(start_time=start_time, end_time=end_time)

        Get custom logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_custom_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("custom", start_time, end_time)

    def get_graphlab_service_logs(self, start_time, end_time):
        """
        Fetch graphlab service logs from S3 corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_feedback_logs, get_server_logs, get_custom_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all query logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get graphlab service logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_graphlab_service_logs(start_time=start_time, end_time=None)

        Get graphlab service logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_graphlab_service_logs(start_time=start_time, end_time=end_time)

        Get graphlab service logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_graphlab_service_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("graphlab_service", start_time, end_time)

    def show(self):
        """
        Visualize the Predictive Service with GraphLab Canvas. This function
        starts Canvas if it is not already running.

        Returns
        -------
        view : graphlab.canvas.view.View
            An object representing the GraphLab Canvas view

        See Also
        --------
        canvas
        """
        self._ensure_not_terminated()

        _canvas.get_target().state.set_selected_variable(('Predictive Services', self.name))
        return _canvas.show()

    def save(self):
        """
        Saves the Predictive Service information to disk. Information saved
        contains: Name, State path, and AWS credentials. Note that only metadata
        of the Predictive Service is stored, it does not impact the actual
        deployed Predictive Service.

        This information can be useful later to access the Predictive Service,
        for example:

            >>> import graphLab
            >>> my_ps = graphlab.deploy.predictive_services[<name>]

        """
        self._ensure_not_terminated()
        self._session.save(self, typename="PredictiveService")

    def cache_enable(self, name=None, restart=True, max_retries=3):
        """
        Enable caching for a Predictive Service.

        Parameters
        ----------
        name : str, optional
            A unique identifier for the Predictive Object. If empty or None, the
            cache will be enabled for the entire service.

        restart : bool, optional (defaults to True)
            A boolean indicating whether the underlying cache process should be
            restarted (and all data erased).
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()

        # update and save cache state
        self._update_and_save_cache_status(name, 'enabled')

        # send control plane query to enable cache
        self._environment.cache_enable(name, restart)

        num_retries = 0
        while not self._environment._is_cache_ok("healthy") and \
          num_retries < max_retries:
            self._environment.cache_enable(name, restart)
            num_retries += 1
            _time.sleep(2)

        if not self._environment._is_cache_ok("healthy"):
            _logger.warn("Cluster cache unstable; disabling")
            self.cache_disable()
        else:
            _logger.info('Cache enabled successfully!')

    def cache_disable(self, name=None):
        """
        Disable caching for a Predictive Service.

        Parameters
        ----------
        name : str, optional
            A unique identifier for the Predictive Object. If empty or None, the
            cache will be disabled for the entire service.
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()

        # update and save cache state
        self._update_and_save_cache_status(name, 'disabled')

        # send control plane query to disable cache
        self._environment.cache_disable(name)

        _logger.info('Cache disabled successfully!')

    def cache_clear(self, name=None):
        """
        Clear a Predictive Service's cache.

        Parameters
        ----------
        name : str, optional
            A unique identifier for the Predictive Object. If a valid name is
            specified, only cache entries associated with that particular
            Predictive Object will be removed. Otherwise, if name is empty or
            None, the entire cache is cleared.
        """
        self._ensure_not_terminated()
        self._environment.cache_clear(name)
        _logger.info('Cache cleared successfully!')

    def flush_logs(self):
        """
        Force a Predictive Service to ship its logs to its environment's log path.

        Logs are shipped periodically, but you may find it useful for diagnosing
        unexpected behavior to ship the logs on demand. The logs will be shipped
        to the location specified in the `EC2Environment` at the time the
        Predictive Service was created.
        """
        self._ensure_not_terminated()
        self._environment.flush_logs()

    def set_CORS(self, cors_origin):
        """
        Sets the CORS origin for the Predictive Service

        Parameters
        ----------
        cors_origin : str
            The string value to use as HTTP header Access-Control-Allow-Origin,
            in order to support Cross-Origin Resource Sharing as described in
            https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS.
            If '' (empty String) is specified, CORS support will be disabled.
            If the string value is '*', CORS support will be enabled for all URIs.
            If the string value is 'https://dato.com', CORS support will be
            enabled for 'https://dato.com' only.

        Notes
        -----
        This operation will be applied immediately and will take effect as
        soon as all nodes are up to date.

        Examples
        --------
        Allow requests from dato.com to access this Predictive Service:

            >>> ps.set_CORS('https://dato.com')

        Allow requests from any origin to access this Predictive Service:

            >>> ps.set_CORS('*')

        Disable CORS for this Predictive Service:

            >>> ps.set_CORS(None)
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()

        # set cors_origin
        if not cors_origin:
            cors_origin = ''
        if not isinstance(cors_origin, basestring):
            raise TypeError('CORS origin must be a String.')
        self._cors_origin = cors_origin

        try:
            self._save_state()
            self._environment.poke()
            _logger.info("CORS origin configuration has been changed, please wait a "\
                "while for all nodes to take effect.")
        except _ConnectionError as e:
            _logger.warn("Unable to connect to running Predictive Service: %s" %
                         (e.message))
        self._update_local_state()

    _BAD_PARAM_RE = _re.compile("an unexpected keyword argument '(.*?)'")

    def _set_state_from_params_dict(self, params):
        try:
            self._system_config = _SystemConfig(**params)
        except TypeError as e:
            if "an unexpected keyword argument" in e.message:
                bad_param_name = \
                  PredictiveService._BAD_PARAM_RE.search(e.message).group(1)
                raise ValueError("Invalid configuration parameter specified: %s"
                                 % bad_param_name)
            else:
                raise


    def reconfigure(self, params):
        """
        Updates specified system configuration parameters for the Predictive
        Service.

        Parameters
        ----------
        params : dict
            A dictionary of configuration parameter names and the corresponding
            values to which they will be set. Possible configuration parameters
            are ```cache_max_memory_mb```, and ```cache_ttl_on_update_secs```.

        Examples
        --------
        Update the maximum memory allocated to the cache to be 4GB:

            >>> ps.reconfigure({"cache_max_memory_mb": 4000})

        Update the TTL for the cache entries on model updates:

            >>> ps.reconfigure({"cache_ttl_on_update_secs": 120})
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()

        if not _config_params_are_valid(params):
            raise RuntimeError("Invalid configuration parameters")

        self._set_state_from_params_dict(params)

        try:
            self._save_state()
            self._environment.reconfigure(self._system_config)
            _logger.info("Configuration updated")
        except ValueError:
            raise
        except _ConnectionError as e:
            _logger.error("Unable to connect to running Predictive Service: %s" %
                         (e.message))
        except Exception as e:
            _logger.error("Unable to reconfigure Predictive Service: %s"
                          % e.message)

    def load_predictive_object(self, po_name):
        '''
        Load a Predictive Object into current session.

        Parameters
        -----------
        po_name : str
            The name of the Predictive Object

        Returns
        --------
        out : PredictiveObject
            The Predictive Object
        '''
        self._ensure_not_terminated()

        if not self._predictive_objects.has_key(po_name):
            raise ValueError('Cannot find Predictive Object with name "%s".' % po_name)

        po_version = self._predictive_objects[po_name]['version']
        po_schema_version = self._predictive_objects[po_name]['schema_version']
        path = self._get_predictive_object_save_path(po_name, po_version)
        return _PredictiveObject.load(path, po_schema_version)

    @_retry()
    def _save_predictive_objects(self):
        # Save any new predictive objects.
        for predictive_object_name in self._local_changes:
            (predictive_object, po_info) = self._local_changes[predictive_object_name]
            if predictive_object:         # if this is not a model deletion:
                save_path = self._get_predictive_object_save_path(predictive_object_name, po_info['version'])
                if _is_s3_path(save_path):
                    predictive_object.save(save_path, self._environment.aws_credentials)
                else:
                    predictive_object.save(save_path)

    def _save_state(self):
        # Dump immutable state data to a config
        state = _ConfigParser(allow_no_value=True)
        state.optionxform = str
        state.add_section(PredictiveService._SERVICE_INFO_SECTION_NAME)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Name', self.name)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Description', self.description)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'API Key', self.api_key)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'CORS Origin', self._cors_origin)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Global Cache State', self._global_cache_state)

        # Save environment, if we have one
        if self._environment:
            state.add_section(PredictiveService._ENVIRONMENT_SECTION_NAME)
            for (key, value) in self._environment._get_state(self._schema_version).iteritems():
                state.set(PredictiveService._ENVIRONMENT_SECTION_NAME, key, value)

        # Save deployment version data to config
        state.add_section(PredictiveService._DEPLOYMENT_SECTION_NAME)
        current_predictive_objects = _copy(self._all_predictive_objects)
        for (model_name, info) in current_predictive_objects.iteritems():
            state.set(PredictiveService._DEPLOYMENT_SECTION_NAME, model_name, \
                _json.dumps({
                    'version':info['version'],
                    'description':info['description'],
                    'cache_state':info['cache_state'],
                    'schema_version': info['schema_version']
                }))

        state.add_section(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING)
        for (model_name, info) in current_predictive_objects.iteritems():
            state.set(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING, model_name, info['docstring'].encode('string_escape'))

        if self._has_remote_state_changed():
            raise IOError("Can not save changes. The Predictive Service has changed. Please "
                          "reload your Predictive Service first")

        # Update the revision number after we have successfully written all predictive objects
        self._revision_number += 1
        state.add_section(self._META_SECTION_NAME)
        state.set(self._META_SECTION_NAME, 'Revision Number', self._revision_number)
        state.set(self._META_SECTION_NAME, 'Schema Version', self._schema_version)

        # Save system config. params
        state.add_section(PredictiveService._SYSTEM_SECTION_NAME)
        self._system_config.set_state_in_config(
            state, PredictiveService._SYSTEM_SECTION_NAME)

        # Write state file to state path
        self._environment._write_state_config(state, self._state_path)

    def _update_local_state(self):
        # Update our state
        current_predictive_objects = _copy(self._all_predictive_objects)
        self._local_changes = {}
        self._predictive_objects = dict(zip(current_predictive_objects.keys(),
            [{
                'version': info['version'],
                'docstring': info['docstring'],
                'cache_state': info['cache_state'],
                'schema_version': info['schema_version'],
                'description': info['description']
             } for info in current_predictive_objects.values()]))


    def _get_predictive_object_save_path(self, predictive_object_name, version_number):
        return self._environment._get_root_path(self._state_path) + \
                '/'.join([PredictiveService._PREDICTIVE_OBJECT_DIR,
                          predictive_object_name, str(version_number)])

    def _get_dependency_save_path(self, predictive_object_name, version_number):
        return self._environment._get_root_path(self._state_path) + \
                '/'.join([PredictiveService._DEPENDENCIES_DIR,
                          predictive_object_name, str(version_number)])

    def _ensure_not_terminated(self):
        if not self._environment:
            raise RuntimeError("This operation is not supported because Predictive Service is already terminated")

    def _ensure_no_local_changes(self):
        if len(self._local_changes) != 0:
            raise RuntimeError("There are local pending changes to this " \
                               "Predictive Service. Please apply or remove " \
                               "all local changes before performing the " \
                               "operation.")

    def _update_and_save_cache_status(self, name, cache_status="enabled"):
        # update cache state
        if not name or name == '':
            # enable global cache state
            self._global_cache_state = cache_status

            # enable all Predictive Object cache states directly in our internal state,
            # since this is not a local PO change.
            for k in self._predictive_objects:
                self._predictive_objects[k]['cache_state'] = cache_status
        else:
            # enable cache for Predictive Object defined in 'name'
            if name in self._predictive_objects:
                self._predictive_objects[name]['cache_state'] = cache_status
            else:
                # name does not exist
                raise ValueError('Cannot find Predictive Object with name "%s".' % name)

        # save cache state
        self._save_state()
        self._update_local_state()

    def _create_custom_po(self, query, description):
        return _CustomQueryPredictiveObject(
            query = query,
            description = description)

    # The following methods are for savng artifacts in local session:
    def __getstate__(self):
        state = {
            'name': self.name,
            'schema_version': self._schema_version,
            'state_path': self._state_path,
            'description': self.description,
        }
        if hasattr(self._environment, 'aws_credentials'):
            state['aws_credentials'] = self._environment.aws_credentials
        return state

    def _get_version(self):
        return PREDICTIVE_SERVICE_SCHEMA_VERSION

    def _get_metadata(self):
        """
        Get the metadata that is managed by the session. This gets displayed
        in the Scoped session and is stored in the session's index file.

        """
        status = 'Yes' if self._modified_since_last_saved == True else 'No'
        return {'Name': self.name,
                'State_path': self._state_path,
                'Type': type(self).__name__,
                'Unsaved changes?' : status,
                'Creation date': self._session_registration_date}

    @classmethod
    def _load_version(cls, unpickler, version):
        """
        An abstract function to implement save for the object in consideration.

        Parameters
        ----------
        unpickler : A GLUnpickler file handle.

        version : Version number as maintained by the class.

        """
        # Load the dump.
        ps_info = unpickler.load()
        # We had issues loading scheme that were before version 3, so we do not support it.
        if ps_info.schema_version < LAST_SUPPORTED_PREDICTIVE_SERVICE_SCHEMA_VERSION:
            raise RuntimeError("Do not support loading version %s of Predictive Service from local session." \
                % ps_info.schema_version)

        aws_credentials = ps_info.aws_credentials if hasattr(ps_info, 'aws_credentials') else {}

        return _gl.deploy.predictive_service._load_imp(
            state_path = ps_info.state_path,
            aws_access_key_id = aws_credentials.get('aws_access_key_id', None) if aws_credentials else None,
            aws_secret_access_key = aws_credentials.get('aws_secret_access_key', None) if aws_credentials else  None)

    def _has_remote_state_changed(self):
        '''
        Returns whether any changes have been made to this Predictive Service remotely.
        '''
        if self._revision_number == 0:
            return False
        config = PredictiveServiceEnvironment._get_state_from_file(self._state_path, self.aws_credentials)
        saved_version_num = config.getint(self._META_SECTION_NAME, 'Revision Number')
        return (self._revision_number != saved_version_num)

    def _raise_error_if_on_premise_ps(self):
        '''
        Raise an exception if the environment is docker.
        '''
        if isinstance(self._environment, _DockerPredictiveServiceEnvironment):
            raise Exception("The function " + _inspect.stack()[1][3] + " is disabled on the" +
                            " client side for on-premises Predictive Service. Please check with your" +
                            " systems admin for further instructions.")
